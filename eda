1. eda_prefect_flow.py:

from prefect import flow, task
import os
import pandas as pd
import json
import time
import hashlib
from datetime import datetime, date
import math

from data_eda.my_ge_agentic_utils import apply_dynamic_expectations
from data_eda.advanced_evaluator import generate_advanced_review
from data_eda.eda_logger import save_eda_log
from data_eda.eda_rule_flexible import EDAReasonerFlexible  # Ganti Gemini dengan EDAReasonerFlexible

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
EDA_LOG_DIR = os.path.join(BASE_DIR, "data_eda", "data_log")
if not os.path.exists(EDA_LOG_DIR):
    os.makedirs(EDA_LOG_DIR)

EDA_RESULT_DIR = os.path.join(BASE_DIR, "data_eda", "eda_result")
if not os.path.exists(EDA_RESULT_DIR):
    os.makedirs(EDA_RESULT_DIR)

TEMPLATE_PATH = os.path.join(
    BASE_DIR, "data_eda", "eda_result", "templates", "eda_pdf_template.html"
)

CONFIG_PATH = os.path.join(BASE_DIR, "data_eda", "config.json")
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        CONFIG = json.load(f)
else:
    CONFIG = {}

def get_conf(*keys, default=None):
    d = CONFIG
    for k in keys:
        if k in d:
            d = d[k]
        else:
            return default
    return d

def json_serial(obj):
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    return str(obj)

def clean_json(obj):
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        else:
            return obj
    elif isinstance(obj, (datetime, date, pd.Timestamp)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    else:
        return obj

def get_file_hash(filepath):
    hash_sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            hash_sha256.update(chunk)
    return hash_sha256.hexdigest()

def get_meta_file_entries(meta_path, data_dir):
    if not os.path.exists(meta_path):
        return []
    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    meta_files = meta.get("files", []) if isinstance(meta, dict) else meta
    file_entries = []
    for entry in meta_files:
        fname = entry.get("saved_name") or entry.get("name")
        file_id = entry.get("id")
        if fname and fname.endswith(".parquet") and file_id:
            fpath = os.path.join(data_dir, fname)
            if os.path.exists(fpath):
                file_entries.append({
                    "id": file_id,
                    "file": fname,
                    "path": fpath,
                })
    return file_entries

def get_existing_eda_hash(file_id):
    json_file = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
    if not os.path.exists(json_file):
        return None
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("data_hash")
    except Exception as e:
        print(f"[ERROR] get_existing_eda_hash: {e}")
        return None

def remove_orphan_eda_results(data_dir, meta_path):
    valid_file_ids = set()
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        meta_files = meta.get("files", []) if isinstance(meta, dict) else meta
        for entry in meta_files:
            file_id = entry.get("id")
            fname = entry.get("saved_name") or entry.get("name")
            if file_id and fname and fname.endswith(".parquet"):
                fpath = os.path.join(data_dir, fname)
                if os.path.exists(fpath):
                    valid_file_ids.add(file_id)
    for fname in os.listdir(EDA_RESULT_DIR):
        if not fname.endswith("_eda_final.json") and not fname.endswith("_eda_final.pdf") and not fname.endswith("_eda_final.html"):
            continue
        parts = fname.split("_eda_final.")
        if len(parts) != 2:
            continue
        file_id = parts[0]
        if file_id not in valid_file_ids:
            try:
                os.remove(os.path.join(EDA_RESULT_DIR, fname))
                print(f"[CLEANUP] Removed orphan EDA result: {fname}")
            except Exception as e:
                print(f"[CLEANUP] Failed to remove {fname}: {e}")

def log_task_status(status_monitor, step, status, error=None, extra=None):
    log = {"step": step, "status": status}
    if error:
        log["error"] = str(error)
    if extra:
        log.update(extra)
    status_monitor.append(log)
    print(f"[TASK_LOG] {log}")

@task
def load_and_analyze_file(filepath, file_id=None, meta_path=None, n_sample=None, frac=None, stratify_col=None, weight_col=None):
    from data_eda.sampling import get_file_sample_df
    from data_eda.eda import advanced_eda
    from data_eda.schemas import EDAResult
    from data_eda.pdf_report_generator import generate_pdf_report

    warnings = []
    sampled = df = None
    status_monitor = []

    basename = os.path.splitext(os.path.basename(filepath))[0]
    output_json_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
    output_pdf_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.pdf")
    output_html_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.html")
    template_path = TEMPLATE_PATH

    data_hash = get_file_hash(filepath)

    try:
        log_task_status(status_monitor, "sampling", "started")
        sample_conf = CONFIG.get('sampling', {})
        sample_df, sampling_info = get_file_sample_df(filepath)
        df = pd.read_parquet(filepath)
        log_task_status(status_monitor, "sampling", "success", extra={"rows": len(df), "cols": list(df.columns)})
    except Exception as e:
        log_task_status(status_monitor, "sampling", "error", error=e)
        return {
            "file": filepath,
            "error": f"Load/Sampling error: {e}",
            "status_monitor": status_monitor
        }

    profile_dict = None
    try:
        log_task_status(status_monitor, "profiling", "started")
        from ydata_profiling import ProfileReport
        profile = ProfileReport(sample_df, minimal=False, explorative=True)
        if hasattr(profile, "to_dict"):
            profile_dict = profile.to_dict()
        elif hasattr(profile, "get_description"):
            profile_dict = profile.get_description()
        elif hasattr(profile, "to_json"):
            profile_dict = json.loads(profile.to_json())
        log_task_status(status_monitor, "profiling", "success")
    except Exception as e:
        log_task_status(status_monitor, "profiling", "error", error=e)
        warnings.append(f"Profiling error: {e}")

    ge_summary = None
    validation_result = None
    try:
        log_task_status(status_monitor, "great_expectations", "started")
        import great_expectations as ge
        from great_expectations.core.batch import RuntimeBatchRequest
        context = ge.get_context()
        batch_request = RuntimeBatchRequest(
            datasource_name="my_pandas_datasource",
            data_connector_name="default_runtime_data_connector_name",
            data_asset_name="my_pandas_asset",
            runtime_parameters={"batch_data": sample_df},
            batch_identifiers={"default_identifier_name": "default_id"},
        )
        datasources = [ds["name"] for ds in context.list_datasources()]
        if "my_pandas_datasource" not in datasources:
            context.add_datasource(
                "my_pandas_datasource",
                class_name="Datasource",
                execution_engine={"class_name": "PandasExecutionEngine"},
                data_connectors={
                    "default_runtime_data_connector_name": {
                        "class_name": "RuntimeDataConnector",
                        "batch_identifiers": ["default_identifier_name"],
                    }
                }
            )
        validator = context.get_validator(batch_request=batch_request)
        validator = apply_dynamic_expectations(validator, sample_df)
        validation_result = validator.validate()
        if isinstance(validation_result, dict):
            ge_summary = {
                "success": validation_result.get("success"),
                "statistics": validation_result.get("statistics"),
                "expectations_failed": validation_result.get("unsuccessful_expectations"),
            }
        else:
            ge_summary = {
                "success": getattr(validation_result, "success", None),
                "statistics": getattr(validation_result, "statistics", None),
                "expectations_failed": getattr(validation_result, "unsuccessful_expectations", None),
            }
        log_task_status(status_monitor, "great_expectations", "success")
    except Exception as e:
        log_task_status(status_monitor, "great_expectations", "error", error=e)
        warnings.append(f"Great Expectations error: {e}")

    anomaly_alerts = []
    anomaly_rate = 0.0

    try:
        log_task_status(status_monitor, "advanced_eda", "started")
        eda_dict = advanced_eda(
            sample_df,
            file=filepath,
            meta=None,
            progress=None,
            sampling_info=sampling_info,
            profiling_dict=profile_dict,
            ge_result=ge_summary,
            kolaborasi_summary=None
        )
        # Tambahkan insight/rekomendasi otomatis dari EDAReasonerFlexible
        try:
            analyzer = EDAReasonerFlexible(sample_df)
            eda_insight = analyzer.get_insight()
            eda_dict["eda_insight"] = eda_insight
        except Exception as e:
            eda_dict["eda_insight"] = {
                "insight": f"EDA insight error: {e}",
                "recommendations": []
            }
        log_task_status(status_monitor, "advanced_eda", "success")
    except Exception as e:
        log_task_status(status_monitor, "advanced_eda", "error", error=e)
        return {
            "file": filepath,
            "error": f"Advanced EDA error: {e}",
            "status_monitor": status_monitor
        }

    eda_dict['anomaly'] = {
        "alerts": anomaly_alerts,
        "rate": anomaly_rate
    }
    eda_dict['warnings'] = warnings

    try:
        log_task_status(status_monitor, "schema", "started")
        from data_eda.schemas import EDAResult
        eda_schema = EDAResult(**eda_dict)
        log_task_status(status_monitor, "schema", "success")
    except Exception as e:
        log_task_status(status_monitor, "schema", "error", error=e)
        return {
            "file": filepath,
            "error": f"Schema validation error: {e}",
            "eda_dict": eda_dict,
            "status_monitor": status_monitor
        }

    try:
        log_task_status(status_monitor, "save_json", "started")
        try:
            output = clean_json(eda_schema.model_dump())
        except AttributeError:
            output = clean_json(eda_schema.dict())
        output["data_hash"] = data_hash
        output["file_id"] = file_id
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False, default=json_serial)
        log_task_status(status_monitor, "save_json", "success", extra={"output_json_path": output_json_path})
    except Exception as e:
        log_task_status(status_monitor, "save_json", "error", error=e)
        return {
            "file": filepath,
            "error": f"Save JSON error: {e}",
            "status_monitor": status_monitor
        }

    try:
        log_task_status(status_monitor, "generate_pdf", "started")
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Template {template_path} tidak ditemukan. Pastikan template ada di folder eda_result/templates.")
        from data_eda.pdf_report_generator import generate_pdf_report
        wkhtmltopdf_path = r"C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe"
        try:
            report_data = eda_schema.model_dump()
        except AttributeError:
            report_data = eda_schema.dict()
        generate_pdf_report(
            report_data,
            output_pdf_path,
            wkhtmltopdf_path=wkhtmltopdf_path,
            template_path=template_path
        )
        log_task_status(status_monitor, "generate_pdf", "success", extra={"output_pdf_path": output_pdf_path})
    except Exception as e:
        log_task_status(status_monitor, "generate_pdf", "error", error=e)
        warnings.append(f"Generate PDF/HTML error: {e}")

    try:
        log_task_status(status_monitor, "eda_log", "started")
        try:
            eda_result_dict = clean_json(eda_schema.model_dump())
        except AttributeError:
            eda_result_dict = clean_json(eda_schema.dict())
        log_content = {
            "file": filepath,
            "basename": basename,
            "run_time": datetime.utcnow().isoformat(),
            "eda_result_path": output_json_path,
            "pdf_report_path": output_pdf_path,
            "html_report_path": output_html_path,
            "warnings": warnings,
            "eda_result": eda_result_dict,
            "status_monitor": status_monitor
        }
        save_eda_log(
            EDA_LOG_DIR,
            filepath,
            log_content,
            log_type="eda"
        )
        log_task_status(status_monitor, "eda_log", "success")
    except Exception as e:
        log_task_status(status_monitor, "eda_log", "error", error=e)
        warnings.append(f"EDA log save error: {e}")

    if meta_path and file_id:
        try:
            log_task_status(status_monitor, "update_meta_status", "skipped")
        except Exception as e:
            log_task_status(status_monitor, "update_meta_status", "error", error=e)

    try:
        eda_result_final = eda_schema.model_dump()
    except AttributeError:
        eda_result_final = eda_schema.dict()

    return {
        "file": filepath,
        "output_json": output_json_path,
        "output_pdf": output_pdf_path,
        "output_html": output_html_path,
        "file_id": file_id,
        "data_hash": data_hash,
        "success": True,
        "warnings": warnings,
        "eda_result": eda_result_final,
        "status_monitor": status_monitor
    }

@flow
def eda_analyze_all_files(
    data_dir: str = None,
    n_sample: int = None,
    frac: float = None,
    stratify_col: str = None,
    weight_col: str = None,
    recommendations: list = None,
    meta_path: str = None,
):
    if data_dir is None:
        data_dir = os.path.join(BASE_DIR, "data")
    if meta_path is None:
        meta_path = os.path.join(data_dir, "other_gdrive_meta.json")

    sample_conf = CONFIG.get('sampling', {})
    start_time = time.time()
    results = []
    errors = 0
    anomaly_sum = 0.0
    num_files = 0

    file_entries = get_meta_file_entries(meta_path, data_dir)
    files_to_process = []
    for entry in file_entries:
        file_id = entry["id"]
        fpath = entry["path"]
        current_hash = get_file_hash(fpath)
        eda_hash = get_existing_eda_hash(file_id)
        if eda_hash != current_hash:
            files_to_process.append({
                "id": file_id,
                "file": entry["file"],
                "path": fpath
            })

    if not files_to_process:
        print("[EDA] Tidak ada file baru/berubah untuk diproses.")
    else:
        print(f"[EDA] File untuk diproses (baru/berubah): {[f['file'] for f in files_to_process]}")

    for entry in files_to_process:
        fpath = entry["path"]
        fname = entry["file"]
        file_id = entry["id"]
        if not os.path.exists(fpath):
            continue
        num_files += 1
        res = load_and_analyze_file.submit(
            fpath,
            file_id=file_id,
            meta_path=meta_path
        )
        results.append(res)
    final_results = [r.result() for r in results]

    # LOG ALL final_results for audit/debug
    try:
        with open(os.path.join(EDA_LOG_DIR, "final_results_debug.json"), "w", encoding="utf-8") as f:
            json.dump([clean_json(x) for x in final_results], f, indent=2, ensure_ascii=False)
        print(f"[DEBUG] Saved all final_results to {os.path.join(EDA_LOG_DIR, 'final_results_debug.json')}")
    except Exception as e:
        print(f"[ERROR] Failed to log all final_results: {e}")

    for r in final_results:
        if not isinstance(r, dict):
            print("[WARNING] Skipping non-dict result in final_results:", r)
            continue
        if r.get("error"):
            errors += 1
        anomaly_sum += r.get("eda_result", {}).get("anomaly", {}).get("rate", 0)
    runtime = time.time() - start_time
    anomaly_rate = anomaly_sum / num_files if num_files else 0

    if recommendations is None:
        recommendations = []
        for r in final_results:
            if not isinstance(r, dict):
                print("[WARNING] Skipping non-dict result in recommendations loop:", r)
                continue
            recs = r.get("eda_result", {}).get("eda_insight", {}).get("recommendations", [])
            if recs:
                recommendations.extend(recs)
        recommendations = list(dict.fromkeys(recommendations)) if recommendations else [
            "Tidak ada rekomendasi dinamis dari EDA insight."
        ]

    remove_orphan_eda_results(data_dir, meta_path)

    all_results = []
    for entry in file_entries:
        file_id = entry["id"]
        json_file = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    all_results.append(json.load(f))
            except Exception as e:
                print(f"[ERROR] Failed reading {json_file}: {e}")

    summary = {
        "timestamp": datetime.utcnow().isoformat(),
        "data_dir": data_dir,
        "num_files": num_files,
        "runtime": runtime,
        "errors": errors,
        "anomaly_rate": anomaly_rate,
        "success_rate": (num_files - errors) / num_files if num_files else 0,
        "details": all_results,
        "recommendations": recommendations,
        "final_results_debug_path": os.path.join(EDA_LOG_DIR, "final_results_debug.json"),
    }
    from data_eda.eda_logger import save_eda_log as save_batch_log
    save_batch_log(
        EDA_LOG_DIR,
        f"batch_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}",
        clean_json(summary),
        log_type="eda_batch",
        with_time=True
    )
    print(f"[SUMMARY_LOG] {json.dumps(summary, indent=2, ensure_ascii=False)}")
    return clean_json(summary)

if __name__ == "__main__":
    eda_analyze_all_files()

2. all_data_backend.py:

import os
import json
import datetime
import pandas as pd
import shutil

from fastapi import FastAPI, Request, Query, HTTPException, BackgroundTasks, status
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

from smart_file_loader import (
    load_all_parquet_tables,
    get_first_parquet_file_path,
)

# --- CENTRALIZED BATCH CONFIG IMPORT ---
from config import (
    START_BATCH_SIZE, MAX_BATCH_SIZE, MIN_BATCH_SIZE,
    TIME_FAST, TIME_SLOW, BATCH_SIZE, get_batch_config
)

from dotenv import load_dotenv
load_dotenv()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

PER_FILE_MAX = START_BATCH_SIZE
TOTAL_MAX = MAX_BATCH_SIZE

GDRIVE_FOLDER_ID_OTHER = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
SERVICE_ACCOUNT_JSON_PATH_OTHER = os.path.join(BASE_DIR, "gdrive_service_account.json")

META_OTHER_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
META_FILES = [META_OTHER_FILE]
EXCLUDE_FILES = {'file_progress.json', 'other_gdrive_meta.json'}
ALLOWED_STATUS_FILE = {"new file", "active", "change", "done", "deleted"}

from utils_gdrive import load_meta

def get_batch_limit_proxy():
    return get_batch_config()

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def try_include_router(module_name, router_name="router"):
    try:
        module = __import__(module_name, fromlist=[router_name])
        router = getattr(module, router_name)
        app.include_router(router)
        print(f"[DEBUG] {module_name} router included")
    except Exception as e:
        print(f"[ERROR] Failed to include {module_name} router: {e}")

try_include_router("scan_data_folder_summary")
try_include_router("upload_frontend_data")

try:
    from monitoring_process import router as monitoring_router
    app.include_router(monitoring_router)
    print("[DEBUG] monitoring_process router included (explicit)")
except Exception as e:
    print(f"[ERROR] Failed to explicitly include monitoring_process router: {e}")

def serialize_for_json(obj):
    if isinstance(obj, (datetime.date, datetime.datetime)):
        return obj.isoformat()
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    return str(obj)

def get_valid_meta_files(meta_file=META_OTHER_FILE, status_file="active"):
    if not os.path.exists(meta_file):
        return []
    meta = load_meta(meta_file)
    if isinstance(meta, dict):
        meta = meta.get("files", [])
    return [
        m.get("saved_name") or m.get("name")
        for m in meta
        if (
            (m.get("saved_name") or m.get("name"))
            and (m.get("status_file", "") == status_file)
            and (m.get("saved_name") or m.get("name")).endswith(".parquet")
            and not (m.get("saved_name") or m.get("name")).endswith('.parquet.meta.json')
        )
    ]

def agentic_sync_meta_to_parquet():
    meta_file = META_OTHER_FILE
    data_dir = DATA_DIR
    if not os.path.exists(meta_file):
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({"files": []}, f, indent=2, ensure_ascii=False)
    meta = load_meta(meta_file)
    meta_files = meta.get("files", meta) if isinstance(meta, dict) else meta
    meta_names = set((entry.get("name") or entry.get("saved_name") or "").strip() for entry in meta_files)
    files_on_disk = [f for f in os.listdir(data_dir)
                     if f.endswith(".parquet") and not f.endswith(".parquet.meta.json")]
    updated = False
    import uuid
    for fname in files_on_disk:
        if fname not in meta_names:
            id_new = str(uuid.uuid4())[:16]
            try:
                nrows = len(pd.read_parquet(os.path.join(data_dir, fname)))
            except Exception:
                nrows = 0
            entry = {
                "name": fname,
                "status_file": "new file",
                "id": id_new,
                "record_count": nrows
            }
            meta_files.append(entry)
            print(f"[META][SYNC][ADD] File {fname} ditambahkan ke meta (id={id_new}, rows={nrows})")
            updated = True
    files_on_disk_set = set(files_on_disk)
    meta_new = []
    for entry in meta_files:
        fname = entry.get("name") or entry.get("saved_name")
        if fname in files_on_disk_set:
            meta_new.append(entry)
        else:
            print(f"[META][SYNC][REMOVE] File {fname} di meta sudah tidak ada di disk, dihapus dari meta")
            updated = True
    if updated:
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({"files": meta_new}, f, indent=2, ensure_ascii=False)
        print(f"[META][SYNC] Meta master updated ({len(meta_new)} files)")
    return updated

@app.get("/config_limits")
def api_config_limits():
    return get_batch_limit_proxy()

@app.get("/list_active_files")
def list_active_files():
    return {"files": get_valid_meta_files()}

def ambil_data(file_name, offset, limit):
    fpath = os.path.join(DATA_DIR, file_name)
    if not os.path.exists(fpath):
        raise HTTPException(status_code=404, detail=f"File {file_name} not found")
    df = pd.read_parquet(fpath)
    return df.iloc[offset:offset+limit].to_dict(orient="records")

@app.get("/")
def root():
    print("[DEBUG] root called")
    return {"message": "FastAPI backend is running!"}

from utils_gdrive import (
    get_gdrive_file_list,
    scan_local_files,
    GDRIVE_FOLDER_ID as UTILS_GDRIVE_FOLDER_ID,
    SERVICE_ACCOUNT_JSON as UTILS_GDRIVE_SERVICE_ACCOUNT_JSON,
    DATA_DIR as UTILS_GDRIVE_DATA_DIR,
)

@app.get("/list_gdrive_files")
async def list_gdrive_files_endpoint():
    try:
        files = get_gdrive_file_list(
            folder_id=UTILS_GDRIVE_FOLDER_ID,
            service_account_json_path=UTILS_GDRIVE_SERVICE_ACCOUNT_JSON
        )
        return {"status": "success", "count": len(files), "files": files}
    except Exception as e:
        print(f"[ERROR][ENDPOINT] /list_gdrive_files: {e}")
        import traceback; traceback.print_exc()
        return {"status": "error", "error": str(e), "files": []}

@app.get("/list_local_files")
async def list_local_files_endpoint():
    try:
        files = scan_local_files(UTILS_GDRIVE_DATA_DIR)
        return {"status": "success", "count": len(files), "files": files}
    except Exception as e:
        print(f"[ERROR][ENDPOINT] /list_local_files: {e}")
        import traceback; traceback.print_exc()
        return {"status": "error", "error": str(e), "files": {}}

@app.post("/trigger_gdrive_sync")
async def trigger_gdrive_sync(request: Request):
    from utils_gdrive import trigger_gdrive_sync
    log = []
    try:
        _ = await request.json() if request.headers.get("content-type", "").startswith("application/json") else None
    except Exception:
        pass
    try:
        print("[DEBUG] trigger_gdrive_sync: Syncing other folder")
        res_other = trigger_gdrive_sync(
            folder_id=GDRIVE_FOLDER_ID_OTHER,
            data_dir=DATA_DIR,
            service_account_json_path=SERVICE_ACCOUNT_JSON_PATH_OTHER,
            meta_prefix="other"
        )
        log.append(f"Synced other folder: {res_other.get('meta_file_main', '')}")
    except Exception as e:
        log.append(f"Failed to sync other: {e}")
        print(f"[DEBUG] trigger_gdrive_sync: Failed to sync other: {e}")
    return JSONResponse({"status": "done", "log": log})

@app.post("/trigger_download_missing_files")
async def trigger_download_missing_files(request: Request):
    from download_gdrive_files import download_missing_files

    log = []
    results = []
    try:
        _ = await request.json() if request.headers.get("content-type", "").startswith("application/json") else None
    except Exception:
        pass

    folder_configs = [
        {
            "meta_prefix": "other",
            "data_dir": DATA_DIR,
            "service_account_json_path": SERVICE_ACCOUNT_JSON_PATH_OTHER,
        }
    ]
    for conf in folder_configs:
        try:
            meta_file_path = os.path.join(conf["data_dir"], "other_gdrive_meta.json")
            res = download_missing_files(
                data_dir=conf["data_dir"],
                meta_path=meta_file_path,
                service_account_json_path=conf["service_account_json_path"]
            )
            results.append(res)
            log.append(f"Downloaded for {conf['meta_prefix']}")
        except Exception as e:
            results.append({"error": str(e)})
            log.append(f"Failed to download for {conf['meta_prefix']}: {e}")
    return {"status": "done", "log": log, "results": results}

@app.post("/sync_meta_only")
def sync_meta_only_endpoint():
    print("[DEBUG] /sync_meta_only called")
    updated = agentic_sync_meta_to_parquet()
    return {"status": "meta_synced", "updated": updated}

from batch_agentic import ProgressManager, run_batch_agentic

@app.get("/progress")
def api_get_progress():
    pm = ProgressManager(DATA_DIR)
    return pm.get_all_progress()

@app.get("/progress/{file_id}")
def api_get_progress_file(file_id: str):
    pm = ProgressManager(DATA_DIR)
    prog = pm.get_file_progress(file_id)
    if not prog:
        raise HTTPException(status_code=404, detail="Not found")
    return {file_id: prog}

@app.post("/batch_run_agentic")
def api_run_batch_agentic(background_tasks: BackgroundTasks):
    def _run():
        print("[DEBUG] /batch_run_agentic background batch start")
        run_batch_agentic()
        print("[DEBUG] /batch_run_agentic background batch finished")
    background_tasks.add_task(_run)
    return {"status": "batch_agentic_started"}

@app.post("/progress/manual-reset")
def api_manual_reset(file_id: str = Query(..., description="Nama file untuk direset progress-nya")):
    pm = ProgressManager(DATA_DIR)
    pm.reset_progress(file_id)
    return {"status": "reset", "file": file_id}

@app.post("/progress/remove")
def api_remove_progress(file_id: str = Query(..., description="Nama file untuk dihapus progress-nya")):
    pm = ProgressManager(DATA_DIR)
    pm.remove_file_progress(file_id)
    return {"status": "removed", "file": file_id}

def _detect_file(tname, tdict, data_dir):
    filename = tdict.get('filename') or tdict.get('file_path') or tdict.get('saved_name') or None
    if filename and os.path.basename(filename):
        filename = os.path.basename(filename)
    else:
        candidates = []
        for f in get_valid_meta_files():
            fname, ext = os.path.splitext(f)
            if fname == tname or f == tname or f.startswith(tname):
                candidates.append(f)
        filename = candidates[0] if candidates else tname
    print(f"[DEBUG] _detect_file: tname={tname}, detected filename={filename}")
    return filename

def collect_tabular_data(data_dir, only_table=None, include_progress=True, only_processed=True):
    print(f"[DEBUG] collect_tabular_data: only_table={only_table}, only_processed={only_processed}")
    tables_parquet = load_all_parquet_tables(data_dir)
    print(f"[DEBUG] collect_tabular_data: loaded tables_parquet={list(tables_parquet.keys())}")
    file_entries = []
    keys = [only_table] if only_table else list(tables_parquet.keys())
    for tname in keys:
        tdict = tables_parquet.get(tname)
        if not tdict:
            continue
        filename = _detect_file(tname, tdict, data_dir)
        if filename in EXCLUDE_FILES:
            print(f"[DEBUG] collect_tabular_data: skipping excluded file {filename}")
            continue
        data = tdict.get('data', [])
        processed = None
        if only_processed:
            try:
                with open(os.path.join(DATA_DIR, "file_progress.json"), "r", encoding="utf-8") as f:
                    progress_map = json.load(f)
                processed = progress_map.get(filename, {}).get("processed", None)
            except Exception:
                processed = None
        if processed is not None and processed > 0:
            filtered_data = data[:processed]
        elif processed is not None and processed == 0:
            filtered_data = []
        else:
            filtered_data = data
        for row in filtered_data:
            row_with_file = dict(row)
            row_with_file['data_file'] = filename
            file_entries.append((tname, tdict, filename, len(filtered_data)))
    file_entries = sorted(file_entries, key=lambda x: x[3])
    merged = []
    for tname, tdict, filename, _ in file_entries:
        data = tdict.get('data', [])
        processed = None
        if only_processed:
            try:
                with open(os.path.join(DATA_DIR, "file_progress.json"), "r", encoding="utf-8") as f:
                    progress_map = json.load(f)
                processed = progress_map.get(filename, {}).get("processed", None)
            except Exception:
                processed = None
        if processed is not None and processed > 0:
            filtered_data = data[:processed]
        elif processed is not None and processed == 0:
            filtered_data = []
        else:
            filtered_data = data
        for row in filtered_data:
            row_with_file = dict(row)
            row_with_file['data_file'] = filename
            merged.append(row_with_file)
    print(f"[DEBUG] collect_tabular_data: merged data length={len(merged)}")
    return merged

def list_all_tables(data_dir):
    print(f"[DEBUG] list_all_tables called")
    tables_parquet = load_all_parquet_tables(data_dir)
    result_tables = list(tables_parquet.keys())
    print(f"[DEBUG] list_all_tables: result_tables={result_tables}")
    return result_tables

@app.get("/list_tables")
def api_list_tables():
    print("[DEBUG] api_list_tables called")
    return JSONResponse(content={"tables": list_all_tables(DATA_DIR)})

@app.get("/all_data_merge")
def api_all_data_merge(
    limit: int = Query(START_BATCH_SIZE, ge=1, le=BATCH_SIZE),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge called: limit={limit}, offset={offset}, table={table}")
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        paged_data = merged[offset:offset+limit]
        print(f"[DEBUG] api_all_data_merge: paged_data length={len(paged_data)}")
        json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        print(f"[all_data_merge][ERROR] {e}")
        return JSONResponse(content=[])

@app.get("/all_data_merge_count")
def api_all_data_merge_count(
    table: str = Query(None)
):
    print("[DEBUG] api_all_data_merge_count called")
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        return {"count": len(merged)}
    except Exception as e:
        print(f"[all_data_merge_count][ERROR] {e}")
        return {"count": 0}

@app.post("/all_data_merge")
@app.put("/all_data_merge")
@app.patch("/all_data_merge")
async def api_all_data_merge_post(
    request: Request,
    limit: int = Query(START_BATCH_SIZE, ge=1, le=BATCH_SIZE),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge_post called: limit={limit}, offset={offset}, table={table}")
    max_size = 100 * 1024 * 1024
    try:
        body = await request.body()
        if len(body) > max_size:
            print("[DEBUG] api_all_data_merge_post: body too large")
            raise HTTPException(status_code=413, detail="Payload too large (max 100 MB)")
        data = await request.json()
        if isinstance(data, list):
            merged = data[offset:offset+limit]
        elif isinstance(data, dict):
            if 'data' in data and isinstance(data['data'], list):
                merged = data['data'][offset:offset+limit]
            else:
                merged = [data][offset:offset+limit]
        else:
            merged = []
        if not merged:
            print("[DEBUG] api_all_data_merge_post: no data in body, fallback to local")
            raise Exception("No data in body, fallback to local")
        json_compatible = json.loads(json.dumps(merged, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        print(f"[all_data_merge_post][ERROR] {e}")
        try:
            merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
            paged_data = merged[offset:offset+limit]
            json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
            return JSONResponse(content=json_compatible)
        except Exception as e2:
            print(f"[all_data_merge_post][HYBRID-FALLBACK] Fallback total failure: {e2}")
            return JSONResponse(content=[])

@app.get("/download_data")
def download_data(table: str = Query(None, description="Nama table/data yang ingin diunduh (tanpa extensi)")):
    print(f"[DEBUG] download_data called: table={table}")
    file_path, file_name, media_type = get_first_parquet_file_path(DATA_DIR, table)
    if not file_path or not os.path.exists(file_path):
        print(f"[DEBUG] download_data: file not found")
        raise HTTPException(status_code=404, detail="File data tidak ditemukan")
    print(f"[DEBUG] download_data: sending file {file_path}")
    return FileResponse(file_path, media_type=media_type, filename=file_name)

def cleanup_orphan_files(data_dir, meta_paths, exclude_files=None):
    if exclude_files is None:
        exclude_files = set()
    else:
        exclude_files = set(exclude_files)
    if isinstance(meta_paths, str):
        meta_paths = [meta_paths]
    expected_files = set()
    for meta_path in meta_paths:
        if not os.path.exists(meta_path):
            continue
        meta = load_meta(meta_path)
        if isinstance(meta, dict):
            meta = meta.get("files", [])
        expected_files.update(m.get("saved_name") or m.get("name") for m in meta if m.get("saved_name") or m.get("name"))
    current_files = set(os.listdir(data_dir))
    protected_files = expected_files | exclude_files
    orphan_files = [f for f in current_files if f not in protected_files]
    deleted = []
    for f in orphan_files:
        file_path = os.path.join(data_dir, f)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.remove(file_path)
                deleted.append(f)
                print(f"[CLEANUP] Deleted orphan file: {f}")
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
                deleted.append(f)
                print(f"[CLEANUP] Deleted orphan folder: {f}")
        except Exception as e:
            print(f"[CLEANUP][ERROR] Failed to delete {f}: {e}")
    return deleted

@app.post("/cleanup_orphan_files")
def cleanup_orphan_files_endpoint():
    meta_paths = META_FILES
    exclude = set(EXCLUDE_FILES)
    for mp in meta_paths:
        exclude.add(os.path.basename(mp))
    deleted = cleanup_orphan_files(DATA_DIR, meta_paths, exclude_files=exclude)
    return {"status": "success", "deleted_files": deleted}

@app.post("/trigger_data_cleaner")
def trigger_data_cleaner_endpoint():
    try:
        from data_cleaner import cleaner_run
    except Exception as e:
        return JSONResponse(content={"error": f"Gagal import cleaner_run: {e}"}, status_code=500)
    try:
        result = cleaner_run()
        return JSONResponse(content=result)
    except Exception as e:
        import traceback; traceback.print_exc()
        return JSONResponse(content={"error": str(e)}, status_code=500)

# === PATCH: Endpoint trigger EDA Prefect Flow v3+ (langsung panggil flow, tanpa agent/deployment) ===
from data_eda.eda_prefect_flow import eda_analyze_all_files

@app.post("/run_eda_prefect_flow", status_code=status.HTTP_202_ACCEPTED)
async def run_eda_prefect_flow(request: Request):
    """
    Trigger EDA Prefect Flow secara langsung (tanpa agent, tanpa deployment).
    Body (JSON):
    {
      "data_dir": "C:\\Users\\ASUS\\kpifinance-api\\backend-python\\data",
      "n_sample": 50,
      "frac": null,
      "stratify_col": null,
      "weight_col": null
    }
    Output: {
      "message": ...,
      "result": ...,
      "status": ...
    }
    """
    try:
        params = await request.json()
    except Exception:
        params = {}

    data_dir = params.get("data_dir", r"C:\Users\ASUS\kpifinance-api\backend-python\data")
    n_sample = params.get("n_sample")
    frac = params.get("frac")
    stratify_col = params.get("stratify_col", None)
    weight_col = params.get("weight_col", None)

    # VALIDASI WAJIB: hanya boleh salah satu terisi
    if n_sample is not None and frac is not None:
        return JSONResponse(
            status_code=400,
            content={
                "error": "Jangan isi keduanya: n_sample dan frac. Pilih salah satu."
            }
        )
    if n_sample is None and frac is None:
        n_sample = 50  # fallback default

    try:
        result = eda_analyze_all_files(
            data_dir=data_dir,
            n_sample=n_sample,
            frac=frac,
            stratify_col=stratify_col,
            weight_col=weight_col,
        )
        # status_monitor dari setiap file
        status_list = [
            {
                "file": detail.get("file"),
                "status_monitor": detail.get("status_monitor", []),
                "success": detail.get("success", False),
                "warnings": detail.get("warnings", []),
                "error": detail.get("error", None),
            }
            for detail in result.get("details", [])
        ]
        return JSONResponse(
            status_code=200,
            content=json.loads(json.dumps({
                "message": "EDA Prefect Flow berhasil dijalankan.",
                "result": result,         
                "status": status_list     
            }, default=serialize_for_json)),
        )
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        return JSONResponse(
            status_code=500,
            content=json.loads(json.dumps({
                "error": str(e),
                "traceback": tb,
                "message": "Terjadi error saat menjalankan EDA Prefect Flow.",
            }, default=serialize_for_json)),
        )

if __name__ == "__main__":
    import uvicorn
    print("[DEBUG] __main__ starting uvicorn")
    uvicorn.run("all_data_backend:app", host="0.0.0.0", port=8000, reload=True, workers=1)

3. eda.py:

import numpy as np
import pandas as pd
import datetime
from typing import Dict, Any, List, Optional
from utils import safe
import os
import json
from scipy import stats  

from data_eda.eda_logger import save_eda_log
from data_eda.eda_rule_flexible import EDAReasonerFlexible

DATA_LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data_eda", "data_log")
os.makedirs(DATA_LOG_DIR, exist_ok=True)

CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data_eda", "config.json")
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        CONFIG = json.load(f)
else:
    CONFIG = {}

def get_conf(*keys, default=None):
    d = CONFIG
    for k in keys:
        if k in d:
            d = d[k]
        else:
            return default
    return d

def clean_json(obj):
    import math
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        else:
            return obj
    elif isinstance(obj, (datetime.datetime, datetime.date, pd.Timestamp)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    else:
        return obj

def suggest_schema_expectation(df: pd.DataFrame) -> Dict[str, Any]:
    result = {}
    for col in df.columns:
        dtype = str(df[col].dtype)
        suggestion = {"type": dtype}
        if pd.api.types.is_numeric_dtype(df[col]):
            suggestion['min'] = float(safe(df[col].min(), 0))
            suggestion['max'] = float(safe(df[col].max(), 0))
            suggestion['n_unique'] = int(df[col].nunique())
        if pd.api.types.is_string_dtype(df[col]):
            nonnull = df[col].dropna().astype(str)
            suggestion['max_length'] = int(nonnull.str.len().max()) if not nonnull.empty else 0
            suggestion['n_unique'] = int(df[col].nunique())
            sample_values = nonnull.unique()[:5].tolist() if not nonnull.empty else []
            suggestion['sample_values'] = sample_values
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            suggestion['min'] = str(safe(df[col].min()))
            suggestion['max'] = str(safe(df[col].max()))
        result[col] = suggestion
    return result

def explain_expectation_applied(col, col_type, rule, params, evidence=None):
    reason = f"Expectation '{rule}' applied to column '{col}' (type: {col_type}) with params {params}."
    if evidence:
        reason += f" Evidence: {evidence}"
    return reason

def capture_data_reference(df, cols=None, n=10):
    bins = get_conf('eda', 'hist_bins', default=10)
    ref = {}
    if cols is None:
        cols = df.columns
    for col in cols:
        ser = df[col]
        if ser.dtype.kind in "biufc":  # numeric
            try:
                hist = np.histogram(ser.dropna(), bins=bins)
                ref[col] = {
                    "min": float(ser.min()) if len(ser.dropna()) else None,
                    "max": float(ser.max()) if len(ser.dropna()) else None,
                    "sample": ser.dropna().sample(n=min(n, len(ser.dropna()))).tolist() if len(ser.dropna()) else [],
                    "hist_bins": hist[1].tolist() if len(hist[1]) else [],
                    "hist_counts": hist[0].tolist() if len(hist[0]) else [],
                }
            except Exception:
                ref[col] = {}
        else:
            ref[col] = {
                "value_sample": ser.dropna().unique()[:n].tolist()
            }
    return ref

def compute_confidence_interval(data, confidence=0.95):
    """
    Compute confidence interval for the mean (robust using t-distribution).
    Returns (lower, upper)
    """
    a = np.array(data.dropna())
    n = len(a)
    if n == 0:
        return (None, None)
    m = np.mean(a)
    se = stats.sem(a) if n > 1 else 0.0
    h = se * stats.t.ppf((1 + confidence) / 2., n-1) if n > 1 else 0.0
    return (float(m - h), float(m + h))

class NumericAnalyzer:
    @staticmethod
    def analyze(df: pd.DataFrame, confidence=0.95) -> Dict[str, Any]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        bins = get_conf('eda', 'hist_bins', default=10)
        result = {}
        for col in numeric_cols:
            s = df[col].dropna()
            desc = s.describe(percentiles=[.25, .5, .75]).to_dict()
            q25 = desc.get('25%')
            q75 = desc.get('75%')
            iqr = q75 - q25 if (q75 is not None and q25 is not None) else None
            hist = np.histogram(s, bins=bins) if len(s) else ([], [])
            outlier_count = int(((s < (q25 - 1.5 * iqr)) | (s > (q75 + 1.5 * iqr))).sum()) if iqr else 0
            skew = float(s.skew()) if len(s) > 2 else 0
            kurt = float(s.kurt()) if len(s) > 2 else 0
            zeros = int((s == 0).sum())
            neg = int((s < 0).sum())
            pos = int((s > 0).sum())
            # Tambahan: Confidence Interval
            ci_low, ci_high = compute_confidence_interval(s, confidence)
            result[col] = {
                "count": float(safe(desc.get("count"), 0)),
                "mean": float(safe(desc.get("mean"), 0)),
                "std": float(safe(desc.get("std"), 0)),
                "min": float(safe(desc.get("min"), 0)),
                "q25": float(safe(desc.get("25%"), 0)),
                "q50": float(safe(desc.get("50%"), 0)),
                "q75": float(safe(desc.get("75%"), 0)),
                "max": float(safe(desc.get("max"), 0)),
                "unique": int(s.nunique()),
                "outlier_count": outlier_count,
                "hist_bins": hist[1].tolist() if len(hist[1]) else [],
                "hist_counts": hist[0].tolist() if len(hist[0]) else [],
                "skewness": skew,
                "kurtosis": kurt,
                "zero_ratio": zeros / len(s) if len(s) else 0,
                "neg_ratio": neg / len(s) if len(s) else 0,
                "pos_ratio": pos / len(s) if len(s) else 0,
                "mean_confidence_interval": [ci_low, ci_high]  # <--- Tambahan metrik robust
            }
        return result

class CategoricalAnalyzer:
    @staticmethod
    def analyze(df: pd.DataFrame, max_freq: int = None) -> Dict[str, Any]:
        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        if max_freq is None:
            max_freq = get_conf('eda', 'max_freq', default=5)
        result = {}
        for col in cat_cols:
            s = df[col].astype(str).fillna('')
            freq = s.value_counts().head(max_freq).to_dict()
            value_counts = s.value_counts(normalize=True)
            entropy = float(value_counts.map(lambda p: -p * np.log2(p) if p > 0 else 0).sum())
            rare = [v for v, c in s.value_counts().items() if c == 1]
            rare_count = len(rare)
            rare_pct = rare_count / len(s) * 100 if len(s) else 0
            result[col] = {
                "unique": int(s.nunique()),
                "top_freq": freq,
                "entropy": entropy,
                "rare_count": rare_count,
                "rare_pct": rare_pct
            }
        return result

class QualityScorer:
    @staticmethod
    def score(eda_results, n_rows):
        non_missing = [1 - (eda_results["missing_per_col"].get(col, 0) / n_rows) for col in eda_results["columns"]]
        completeness_score = round(sum(non_missing) / len(non_missing) * 100, 2) if non_missing else 0.0
        dupe_penalty = max(0, 1 - (eda_results.get("duplicate_pct", 0)/100))
        outlier_penalty = 1.0
        outlier_fracs = []
        for col, stat in eda_results.get("numeric", {}).items():
            n = n_rows
            outlier_count = stat.get("outlier_count", 0)
            frac = outlier_count / n if n else 0
            outlier_fracs.append(frac)
        outlier_thresh = get_conf('eda', 'outlier_threshold', default=0.05)
        if outlier_fracs:
            avg_outlier_frac = np.mean([f for f in outlier_fracs if f > outlier_thresh]) if any(f > outlier_thresh for f in outlier_fracs) else 0
            if avg_outlier_frac > 0:
                outlier_penalty = 1 - min(avg_outlier_frac, 0.3)
        score = completeness_score/100 * dupe_penalty * outlier_penalty
        confidence_score = round(min(max(score, 0), 1) * 100, 2)
        return completeness_score, confidence_score

def advanced_eda(
    df: pd.DataFrame,
    file: str,
    meta: Optional[dict] = None,
    progress: Optional[dict] = None,
    sampling_info: Optional[dict] = None,
    profiling_dict: Optional[dict] = None,
    ge_result: Optional[dict] = None,
    kolaborasi_summary: Optional[dict] = None
) -> dict:
    n = len(df)
    auto_schema = suggest_schema_expectation(df)
    data_reference = capture_data_reference(df)
    numeric_stats = NumericAnalyzer.analyze(df)
    categorical_stats = CategoricalAnalyzer.analyze(df)
    completeness_score, confidence_score = QualityScorer.score(
        {
            "missing_per_col": df.isnull().sum().to_dict(),
            "columns": list(df.columns),
            "duplicate_pct": round(df.duplicated().mean() * 100, 2),
            "numeric": numeric_stats
        },
        n
    )
    sampling_warning = ""
    if sampling_info and "reasoning" in sampling_info and isinstance(sampling_info["reasoning"], dict):
        if "evidently" in sampling_info["reasoning"]:
            sampling_info["representativity_check"] = sampling_info["reasoning"]["evidently"]
            if sampling_info["reasoning"]["evidently"].get("success") is False:
                sampling_warning = "Sample tidak representatif untuk beberapa kolom (lihat reasoning.evidently.summary)."
        if "sampling_warning" in sampling_info["reasoning"]:
            sampling_warning += "\n" + sampling_info["reasoning"]["sampling_warning"]

    result = {
        "file": file,
        "total_rows": n,
        "columns": list(df.columns),
        "columns_count": len(df.columns),
        "duplicate_rows": int(df.duplicated().sum()),
        "duplicate_pct": round(df.duplicated().mean() * 100, 2),
        "missing_per_col": df.isnull().sum().to_dict(),
        "missing_pct_per_col": (df.isnull().mean() * 100).round(2).to_dict(),
        "numeric": numeric_stats,
        "categorical": categorical_stats,
        "completeness_score": completeness_score,
        "confidence_score": confidence_score,
        "auto_schema_expectation": auto_schema,
        "data_reference": data_reference,
        "sampling": sampling_info or {},
        "sampling_warning": sampling_warning.strip(),
        "meta": meta,
        "progress": progress,
        "generated_at": datetime.datetime.now().isoformat()
    }

    # --- ADVANCED ENRICHMENT ---
    try:
        from data_eda.data_drift import compute_stats, load_baseline_stats, detect_drift, save_baseline_stats
        baseline_path = os.path.join(DATA_LOG_DIR, "baseline_stats.json")
        current_stats = compute_stats(df)
        baseline = load_baseline_stats(baseline_path=baseline_path)
        drift_threshold = get_conf('eda', 'drift_threshold', default=0.15)
        if baseline:
            drift_alerts = detect_drift(current_stats, baseline, threshold=drift_threshold)
        else:
            save_baseline_stats(current_stats, baseline_path=baseline_path)
            drift_alerts = []
        result['drift_alerts'] = drift_alerts
    except Exception as e:
        result['drift_alerts'] = [f"Drift detection error: {e}"]

    try:
        from data_eda.feature_interaction import analyze_feature_interaction
        corr_threshold = get_conf('eda', 'corr_threshold', default=0.7)
        vif_threshold = get_conf('eda', 'vif_threshold', default=5.0)
        feature_interaction = analyze_feature_interaction(df)
        feature_interaction['high_correlation'] = [
            p for p in feature_interaction.get('high_correlation', [])
            if p['corr'] > corr_threshold
        ]
        feature_interaction['high_vif'] = [
            v for v in feature_interaction.get('high_vif', [])
            if v['vif'] > vif_threshold
        ]
        result['feature_interaction'] = feature_interaction
    except Exception as e:
        result['feature_interaction'] = {"error": str(e)}

    try:
        from data_eda.timeseries_profile import analyze_timeseries
        result['timeseries_profile'] = analyze_timeseries(df)
    except Exception as e:
        result['timeseries_profile'] = {"error": str(e)}

    try:
        from data_eda.plugins.plugin_registry import run_plugins
        result['plugin_results'] = run_plugins(df)
    except Exception as e:
        result['plugin_results'] = {"error": str(e)}

    try:
        ge_suite_export = get_conf('eda', 'export_ge_suite', default=False)
        if ge_suite_export:
            from data_eda.ge_suite_tools import suggest_schema_to_ge_suite, save_ge_suite
            suite = suggest_schema_to_ge_suite(auto_schema)
            suite_path = os.path.join(DATA_LOG_DIR, "auto_suite.json")
            save_ge_suite(suite, suite_path)
            result['ge_suite_path'] = suite_path
    except Exception as e:
        result['ge_suite_path'] = f"GE Suite Export error: {e}"

    if profiling_dict is not None:
        result["profile"] = profiling_dict
    if ge_result is not None:
        result["ge"] = ge_result
    if kolaborasi_summary is not None:
        result["rangkuman_kolaborasi"] = kolaborasi_summary

    # --- ENRICHMENT: EDA Insight & Recommendations ---
    try:
        analyzer = EDAReasonerFlexible(df)
        eda_insight = analyzer.get_insight()
        result["eda_insight"] = eda_insight
    except Exception as e:
        result["eda_insight"] = {
            "insight": f"EDA insight enrichment error: {e}",
            "recommendations": []
        }

    # --- REVISED LOGGING: Save EDA log per file ---
    try:
        log_content = {
            "file": file,
            "run_time": datetime.datetime.utcnow().isoformat(),
            "eda_result": clean_json(result),
            "completeness_score": completeness_score,
            "confidence_score": confidence_score,
            "duplicate_pct": result["duplicate_pct"],
            "anomaly": {
                "outlier_frac": [
                    stat.get("outlier_count", 0) / n if n else 0
                    for stat in numeric_stats.values()
                ] if numeric_stats else []
            },
            "scores": {
                "completeness": completeness_score,
                "confidence": confidence_score
            },
            "auto_schema": auto_schema
        }
        save_eda_log(DATA_LOG_DIR, file, log_content, log_type="eda")
        save_eda_log(DATA_LOG_DIR, file, clean_json(result), log_type="eda_advanced")
    except Exception as e:
        print(f"[EDA_LOG_ERROR] {e}")

    return result

def batch_eda_on_folder(
    data_dir: str,
    enrich_meta: bool = True,
    n_sample: int = None,
    frac: float = None,
    stratify_col: str = None,
    weight_col: str = None
) -> List[dict]:
    from utils import get_all_parquet_files, enrich_files_with_metadata
    from sampling import get_file_sample_df

    files = get_all_parquet_files(data_dir)
    if enrich_meta:
        files = enrich_files_with_metadata(files)
    all_eda = []
    for f in files:
        try:
            fname = f["name"]
            meta = f.get("meta", {})
            progress = f.get("progress", {})
            sample_df, reasoning = get_file_sample_df(
                f["path"],
                n_sample=n_sample,
                frac=frac
            )
            sampling_info = {
                "sample_shape": sample_df.shape,
                "reasoning": reasoning,
                "stratify_col": stratify_col,
                "weight_col": weight_col,
                "frac": frac
            }
            eda = advanced_eda(
                sample_df,
                file=fname,
                meta=meta,
                progress=progress,
                sampling_info=sampling_info
            )
            all_eda.append(eda)
        except Exception as e:
            err_content = {
                "error": str(e),
                "file": f.get("name"),
                "run_time": datetime.datetime.utcnow().isoformat()
            }
            save_eda_log(DATA_LOG_DIR, f.get("name"), err_content, log_type="eda_error")
    return all_eda

def test_advanced_eda():
    df = pd.DataFrame({
        "cat": ["a", "a", "b", "b", "c"]*20,
        "num": np.arange(100),
        "dt": pd.date_range("2020-01-01", periods=100)
    })
    res = advanced_eda(df, "dummy.parquet")
    assert isinstance(res, dict)
    assert "completeness_score" in res
    assert "confidence_score" in res
    assert "auto_schema_expectation" in res
    assert "data_reference" in res
    # Tambahan: pastikan confidence_interval ada di numeric
    assert "mean_confidence_interval" in res["numeric"]["num"]
    print("advanced_eda() output:", res)

if __name__ == "__main__":
    test_advanced_eda()
    print("eda.py tests OK")

4. advanced_evaluator.py:

import pandas as pd
import json
import os
from dotenv import load_dotenv

from data_eda.eda_rule_flexible import EDAReasonerFlexible

# Muat variabel lingkungan dari file .env
load_dotenv()

def rule_based_insight(df):
    """
    Insight sederhana berbasis rule (opsional, sebagai fallback jika EDAReasonerFlexible gagal).
    """
    insights = []
    if df.isnull().sum().any():
        insights.append("Terdapat nilai kosong (missing value) pada beberapa kolom, lakukan imputasi atau hapus baris terkait.")
    for col in df.columns:
        if df[col].nunique() > 0.9 * len(df):
            insights.append(f"Kolom '{col}' memiliki kardinalitas tinggi (high cardinality), sebaiknya tidak digunakan sebagai fitur prediktor.")
    return insights

def generate_advanced_review(df, profiling_json, ge_summary, gemini_api_key=None):
    """
    Menghasilkan advanced review JSON dengan insight otomatis dari EDAReasonerFlexible.
    Jika output EDAReasonerFlexible gagal, fallback ke insight sederhana rule-based.
    Parameter gemini_api_key diabaikan (tidak relevan lagi).
    """
    try:
        analyzer = EDAReasonerFlexible(df)
        enrichment = analyzer.get_insight()
        if enrichment and ("insight" in enrichment or "recommendations" in enrichment):
            return enrichment
        else:
            return {
                "error": "EDAReasonerFlexible tidak menghasilkan output yang diharapkan.",
                "rule_based_insight": rule_based_insight(df)
            }
    except Exception as e:
        return {
            "error": f"Terjadi kesalahan: {str(e)}",
            "rule_based_insight": rule_based_insight(df)
        }

##########################
# Integrasi dengan sampling.py
##########################

def get_sampled_review(
    fpath,
    profiling_json,
    ge_summary,
    n_sample=None,
    frac=None,
    stratify_col=None,
    balanced_col=None,
    cluster_cols=None,
    n_clusters=10,
    random_state=42,
    gemini_api_key=None,
    sampling_validate=True
):
    """
    Wrapper untuk mengambil sample representatif (menggunakan sampling.py),
    lalu menjalankan advanced review EDAReasonerFlexible berbasis sample.
    """
    from sampling import get_file_sample_df

    # Ambil sample representatif
    sample_df, sampling_info = get_file_sample_df(
        fpath=fpath,
        n_sample=n_sample,
        frac=frac,
        stratify_col=stratify_col,
        balanced_col=balanced_col,
        cluster_cols=cluster_cols,
        n_clusters=n_clusters,
        random_state=random_state,
        validate=sampling_validate
    )

    # Dapatkan advanced review dari sample
    review = generate_advanced_review(
        sample_df,
        profiling_json=profiling_json,
        ge_summary=ge_summary,
        gemini_api_key=gemini_api_key
    )

    # Sertakan info sampling untuk audit trail dan interpretasi insight
    review["sampling_info"] = sampling_info
    return review

# --- Unit Test ---
def test_advanced_evaluator():
    df = pd.DataFrame({
        "A": [1, 2, 3, None, 5, 6, 7, 8, 9, 10],
        "B": ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]
    })
    profiling_json = {"summary": "Dummy profiling"}
    ge_summary = {"checks": "Dummy GE"}
    insight = generate_advanced_review(df, profiling_json, ge_summary)
    print("Advanced review output:", insight)

if __name__ == "__main__":
    test_advanced_evaluator()
    print("advanced_evaluator.py tests OK")

5. eda_logger.py:

import os
import json
import datetime

def get_eda_log_path(
    log_dir, 
    file_data, 
    log_type="eda", 
    suffix=None, 
    with_time=False
):
    """
    Generate log file path with clear naming:
    eda_log_<basename>_<YYYYMMDD>[__<suffix>].json
    If with_time=True, use eda_log_<basename>_<YYYYMMDD_%H%M%S>[__<suffix>].json
    For batch log (file_data = "batch_..."), the basename is batch_...
    """
    base = os.path.splitext(os.path.basename(file_data))[0]
    if with_time:
        date = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    else:
        date = datetime.datetime.utcnow().strftime("%Y%m%d")
    suffix_part = f"__{suffix}" if suffix else ""
    fname = f"{log_type}_log_{base}_{date}{suffix_part}.json"
    return os.path.join(log_dir, fname)

def ensure_json_serializable(obj):
    """
    Ensure obj is JSON serializable, converting datetime/date/etc if needed.
    """
    def default(o):
        if isinstance(o, (datetime.datetime, datetime.date)):
            return o.isoformat()
        return str(o)
    try:
        # Try to dump and load to enforce JSON-compatibility
        return json.loads(json.dumps(obj, ensure_ascii=False, default=default))
    except Exception as e:
        return {"error": f"Object not serializable: {str(e)}", "original": str(obj)}

def save_eda_log(
    log_dir, 
    file_data, 
    log_content, 
    log_type="eda", 
    suffix=None, 
    with_time=False
):
    """
    Save EDA log or metadata log with best practice (overwrite per run).
    For frequent logs (batch/error), set with_time=True to avoid overwrite within a day.
    """
    os.makedirs(log_dir, exist_ok=True)
    log_path = get_eda_log_path(
        log_dir, file_data, log_type, suffix, with_time=with_time
    )
    serializable_log_content = ensure_json_serializable(log_content)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(serializable_log_content, f, indent=2, ensure_ascii=False)
    print(f"[LOG] Saved {log_type} log: {log_path}")
    return log_path

# Example usage for EDAReasonerFlexible output:
# from data_eda.eda_rule_flexible import EDAReasonerFlexible
# df = ... # your pandas DataFrame
# analyzer = EDAReasonerFlexible(df)
# eda_insight = analyzer.get_insight()
# log_content = {"eda_insight": eda_insight}
# save_eda_log("/path/to/data_eda/data_log", "table_orders.parquet", log_content, log_type="eda")
# save_eda_log("/path/to/data_eda/data_log", "batch_run", batch_log_content, log_type="eda_batch", with_time=True)

6. eda_rule_flexible.py:

import pandas as pd
import numpy as np
import re

class EDAReasonerFlexible:
    def __init__(self, df: pd.DataFrame, feedback_path: str = "eda_feedback.txt"):
        self.df = df
        self.insights = []
        self.recommendations = []
        self.feedback_rules = []
        self.feedback_path = feedback_path
        self.load_feedback()

    def load_feedback(self):
        try:
            with open(self.feedback_path, "r", encoding="utf-8") as f:
                self.feedback_rules = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            self.feedback_rules = []

    def _apply_feedback(self):
        for rule in self.feedback_rules:
            # Pattern 1: Jangan drop kolom jika missing value banyak
            if re.search(r"jangan.*drop.*missing", rule, re.IGNORECASE) or \
               re.search(r"missing.*jangan.*drop", rule, re.IGNORECASE):
                self.recommendations = [
                    r if "drop" not in r.lower() else "Imputasi missing value sesuai permintaan user"
                    for r in self.recommendations
                ]
            # Pattern 2: Imbalance, cukup kasih warning
            if re.search(r"imbalance.*warning", rule, re.IGNORECASE):
                self.recommendations = [
                    r for r in self.recommendations if "balanc" not in r.lower()
                ]
            # Pattern 3: Korelasi tinggi, cek ke user bisnis dulu sebelum hapus
            if re.search(r"korelasi.*user bisnis", rule, re.IGNORECASE):
                self.recommendations.append(
                    "Cek domain/user bisnis sebelum menghapus kolom berkorelasi tinggi, sesuai feedback user."
                )
            # Pattern 4: Outlier pada kolom tertentu diabaikan
            m = re.match(r"\s*outlier di kolom ([A-Za-z0-9_]+).*abaikan", rule, re.IGNORECASE)
            if m:
                col = m.group(1)
                self.insights = [
                    i for i in self.insights if col not in i or "outlier" not in i.lower()
                ]
                self.recommendations = [
                    r for r in self.recommendations if col not in r or "outlier" not in r.lower()
                ]
            # Pattern 5: Kolom wajib selalu ada di insight (force show)
            m2 = re.match(r".*wajib tampilkan insight untuk kolom ([A-Za-z0-9_]+)", rule, re.IGNORECASE)
            if m2:
                col = m2.group(1)
                if not any(col in i for i in self.insights):
                    self.insights.append(f"Insight tambahan: Kolom '{col}' wajib ditampilkan sesuai instruksi user.")
            # Pattern 6: Semua insight/rekomendasi tampil dalam bahasa Inggris
            if re.search(r"bahasa inggris", rule, re.IGNORECASE):
                self.insights = [self._translate(i, "en") for i in self.insights]
                self.recommendations = [self._translate(r, "en") for r in self.recommendations]
            # Pattern 7: Custom, tambahkan insight/rekomendasi dari instruksi langsung
            m3 = re.match(r"tambahkan insight: (.+)", rule, re.IGNORECASE)
            if m3:
                self.insights.append(m3.group(1))
            m4 = re.match(r"tambahkan rekomendasi: (.+)", rule, re.IGNORECASE)
            if m4:
                self.recommendations.append(m4.group(1))
            # Tambahkan pattern lain di sini sesuai kebutuhan

    def _translate(self, text, lang):
        # Placeholder: bisa diintegrasi Google Translate API, di sini hanya contoh dummy
        if lang == "en":
            return "[EN] " + text
        return text

    def get_insight(self):
        self.load_feedback()  # Selalu baca feedback terbaru
        # --- EDA utama (seperti sebelumnya, ringkas) ---
        missing = self.df.isnull().mean()
        missing_cols = missing[missing > 0]
        for col, ratio in missing_cols.items():
            self.insights.append(f"Kolom '{col}' memiliki {ratio:.1%} missing value.")
            if ratio > 0.5:
                self.recommendations.append(f"Kolom '{col}' sebaiknya di-drop karena >50% data hilang.")
            else:
                self.recommendations.append(f"Imputasi missing value di kolom '{col}' dengan median/modus/algoritma lain.")

        num_cols = self.df.select_dtypes(include=np.number).columns
        for col in num_cols:
            col_data = self.df[col].dropna()
            if len(col_data) < 5:
                continue
            q1 = col_data.quantile(0.25)
            q3 = col_data.quantile(0.75)
            iqr = q3 - q1
            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr
            outlier_ratio = ((col_data < lower) | (col_data > upper)).mean()
            if outlier_ratio > 0.05:
                self.insights.append(f"Kolom numerik '{col}' memiliki {outlier_ratio:.1%} outlier (IQR).")
                self.recommendations.append(f"Pertimbangkan winsorizing, clipping, atau transformasi pada '{col}'.")

        cat_cols = self.df.select_dtypes(include=["object", "category"]).columns
        for col in cat_cols:
            n_unique = self.df[col].nunique(dropna=True)
            top_freq = self.df[col].value_counts(normalize=True, dropna=True)
            if n_unique > 25:
                self.insights.append(f"Kolom kategorik '{col}' memiliki cardinality tinggi ({n_unique} kategori unik).")
                self.recommendations.append(f"Pertimbangkan grouping/feature engineering pada '{col}'.")
            if not top_freq.empty and top_freq.iloc[0] > 0.9:
                self.insights.append(f"Kolom '{col}' sangat imbalance ({top_freq.index[0]}: {top_freq.iloc[0]:.1%}).")
                self.recommendations.append(f"Lakukan balancing atau teknik sampling jika '{col}' adalah target.")

        if len(num_cols) > 1:
            corr = self.df[num_cols].corr().abs()
            for i, col1 in enumerate(num_cols):
                for col2 in num_cols[i+1:]:
                    val = corr.loc[col1, col2]
                    if val > 0.85:
                        self.insights.append(f"'{col1}' dan '{col2}' sangat berkorelasi (corr={val:.2f}).")
                        self.recommendations.append(f"Pertimbangkan untuk menghapus salah satu dari '{col1}' atau '{col2}'.")

        # Terapkan feedback user
        self._apply_feedback()

        if not self.insights:
            self.insights = ["Tidak ditemukan masalah kualitas data yang signifikan."]
        if not self.recommendations:
            self.recommendations = ["Data siap untuk tahap selanjutnya."]
        return {
            "insight": self.insights,
            "recommendations": self.recommendations
        }

# Contoh pemakaian
if __name__ == "__main__":
    df = pd.DataFrame({
        "A": [1, 2, 3, 100, np.nan, 2, 3, 4, 2, 1, 3, 4],
        "B": [1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7],
        "C": ["a", "b", "b", "a", "a", "a", "a", "b", "c", "a", "b", "a"],
        "D": ["x"] * 12
    })
    analyzer = EDAReasonerFlexible(df)
    result = analyzer.get_insight()
    import json
    print(json.dumps(result, indent=2, ensure_ascii=False))

7. pdf_report_generator.py:

import matplotlib.pyplot as plt
from jinja2 import Environment, FileSystemLoader, select_autoescape
import pdfkit
import os

def generate_plots(df, out_dir):
    plot_paths = {}
    if 'totalsales' in df.columns:
        plt.figure(figsize=(8,5))
        df['totalsales'].plot(kind='hist', bins=20, color='#2196f3', edgecolor='black')
        plt.title('Distribusi Total Penjualan')
        plt.xlabel('Total Penjualan')
        plt.ylabel('Frekuensi')
        plot_sales = os.path.join(out_dir, 'sales_hist.png')
        plt.savefig(plot_sales, bbox_inches='tight')
        plt.close()
        plot_paths['sales_hist'] = plot_sales
    if 'qty' in df.columns:
        plt.figure(figsize=(8,5))
        df['qty'].plot(kind='hist', bins=20, color='#4caf50', edgecolor='black')
        plt.title('Distribusi Jumlah Barang (Qty)')
        plt.xlabel('Qty')
        plt.ylabel('Frekuensi')
        plot_qty = os.path.join(out_dir, 'qty_hist.png')
        plt.savefig(plot_qty, bbox_inches='tight')
        plt.close()
        plot_paths['qty_hist'] = plot_qty
    if 'kategori' in df.columns:
        plt.figure(figsize=(7,7))
        df['kategori'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=plt.cm.Pastel1.colors)
        plt.title('Proporsi Kategori')
        plot_kat = os.path.join(out_dir, 'kategori_pie.png')
        plt.savefig(plot_kat, bbox_inches='tight')
        plt.close()
        plot_paths['kategori_pie'] = plot_kat
    return plot_paths

def generate_pdf_report(
    df, 
    advanced_review, 
    pdf_path,
    profiling=None,
    ge=None,
    eda_insight=None,   # <--- Parameter insight baru
    kolaborasi=None,
    wkhtmltopdf_path=None,
    template_path=None
):
    """
    Membuat PDF report EDA berbasis insight otomatis (EDAReasonerFlexible).
    Parameter gemini dihapus, diganti eda_insight.
    """
    out_dir = os.path.dirname(pdf_path)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    plot_paths = generate_plots(df, out_dir)
    
    # Template logic
    if template_path is None:
        template_dir = out_dir
        template_filename = 'eda_pdf_template.html'
        template_path = os.path.join(template_dir, template_filename)
    else:
        template_dir = os.path.dirname(template_path)
        template_filename = os.path.basename(template_path)

    if not os.path.exists(template_path):
        raise FileNotFoundError(f"Template {template_path} tidak ditemukan. Pastikan template tersedia di {template_dir}")

    env = Environment(
        loader=FileSystemLoader(template_dir),
        autoescape=select_autoescape(['html', 'xml'])
    )
    template = env.get_template(template_filename)
    html_out = template.render(
        review=advanced_review,
        plot_sales=plot_paths.get('sales_hist'),
        plot_qty=plot_paths.get('qty_hist'),
        plot_kategori=plot_paths.get('kategori_pie'),
        profiling=profiling,
        ge=ge,
        eda_insight=eda_insight,   # <--- Tambahkan ke template context
        kolaborasi=kolaborasi
    )
    # Simpan HTML di folder eda_result
    html_out_path = os.path.splitext(pdf_path)[0] + ".html"
    with open(html_out_path, "w", encoding="utf-8") as f:
        f.write(html_out)
    # Convert HTML ke PDF dengan konfigurasi wkhtmltopdf
    if wkhtmltopdf_path is None:
        wkhtmltopdf_path = r"C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe"
    if not os.path.exists(wkhtmltopdf_path):
        raise FileNotFoundError(f"wkhtmltopdf.exe tidak ditemukan di {wkhtmltopdf_path}. Pastikan sudah terinstall.")
    config = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_path)
    try:
        pdfkit.from_string(html_out, pdf_path, configuration=config)
    except Exception as e:
        print(f"ERROR saat generate PDF: {e}")
        raise

if __name__ == "__main__":
    import pandas as pd
    df = pd.DataFrame({
        "totalsales": [100, 200, 150, 300, 350, 400, 120, 180, 170, 260],
        "qty": [1, 2, 2, 3, 1, 2, 2, 1, 3, 2],
        "kategori": ["A", "B", "A", "C", "B", "B", "C", "A", "A", "C"]
    })
    advanced_review = {
        "file": "dummy.parquet",
        "confidence_score": 98.7,
        "completeness_score": 95.4,
        "columns": ["totalsales", "qty", "kategori"],
        "numeric": {
            "totalsales": {
                "mean": 223.0, "std": 95.7, "min": 100, "max": 400
            }
        },
        "categorical": {
            "kategori": {
                "unique": 3,
                "top_freq": {"A": 4, "B": 3, "C": 3}
            }
        },
        "sampling": {
            "sample_shape": (10, 3),
            "reasoning": {"strategy": "random"}
        }
    }
    profiling = {"summary": "Ini hasil profiling dari YData Profiling."}
    ge = {"checks": "Validasi GE lolos semua."}
    eda_insight = {
        "insight": [
            "Kolom 'qty' memiliki distribusi miring ke kanan.",
            "Tidak ditemukan outlier pada kolom 'totalsales'."
        ],
        "recommendations": [
            "Lakukan normalisasi pada kolom 'qty' sebelum modeling."
        ]
    }
    kolaborasi = {"ringkasan": "Ringkasan kolaborasi tim."}

    out_dir = "data_eda/eda_result"
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    pdf_path = os.path.join(out_dir, "dummy_eda_report.pdf")
    generate_pdf_report(df, advanced_review, pdf_path, profiling, ge, eda_insight, kolaborasi)
    print(f"PDF report generated: {pdf_path}")

8. sampling.py:

import pandas as pd
import numpy as np
from typing import Optional, Tuple, Dict, Any, List
import os
import json

# --- OPTIONAL: Use best practice logger if available ---
# from data_eda.eda_logger import save_eda_log

DATA_LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data_eda", "data_log")
if not os.path.exists(DATA_LOG_DIR):
    os.makedirs(DATA_LOG_DIR)

# Load config.json (agentic sampling config)
CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json")
def load_sampling_config() -> dict:
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "r") as f:
            config = json.load(f)
            return config.get("sampling", {})
    return {}

SAMPLING_CONFIG = load_sampling_config()

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.cluster import KMeans

try:
    from imblearn.under_sampling import RandomUnderSampler
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False

try:
    from evidently.test_suite import TestSuite
    from evidently.tests import TestColumnDistribution
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False

from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

##############################
# Auto Feature Selector (Super Cerdas)
##############################
def auto_feature_selector(
    df: pd.DataFrame, 
    target: str, 
    task: str = 'auto', 
    top_n: int = 10, 
    random_state: int = 42,
    use_shap: bool = True
):
    X = df.drop(columns=[target]).select_dtypes(include=['number', 'category', 'object', 'bool']).copy()
    y = df[target]

    if task == 'auto':
        if y.nunique() <= 10 and y.dtype in ['int', 'category', 'object']:
            task = 'classification'
        else:
            task = 'regression'

    for col in X.select_dtypes(include=['object', 'category']):
        X[col] = X[col].astype('category').cat.codes

    info = {}
    selected_features = []

    try:
        if task == 'classification':
            mi = mutual_info_classif(X, y, discrete_features='auto', random_state=random_state)
        else:
            mi = mutual_info_regression(X, y, discrete_features='auto', random_state=random_state)
        mi_scores = pd.Series(mi, index=X.columns)
        info['mutual_info'] = mi_scores.sort_values(ascending=False).to_dict()
    except Exception as e:
        info['mutual_info_error'] = str(e)
        mi_scores = pd.Series(0, index=X.columns)

    try:
        if task == 'classification':
            rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
        else:
            rf = RandomForestRegressor(n_estimators=100, random_state=random_state)
        rf.fit(X, y)
        fi_scores = pd.Series(rf.feature_importances_, index=X.columns)
        info['feature_importance'] = fi_scores.sort_values(ascending=False).to_dict()
    except Exception as e:
        info['feature_importance_error'] = str(e)
        fi_scores = pd.Series(0, index=X.columns)

    if use_shap and SHAP_AVAILABLE:
        try:
            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X)
            if isinstance(shap_values, list):
                mean_abs_shap = pd.DataFrame(shap_values).abs().mean(axis=(0,1))
            else:
                mean_abs_shap = pd.DataFrame(shap_values).abs().mean(axis=0)
            shap_scores = pd.Series(mean_abs_shap, index=X.columns)
            info['shap_importance'] = shap_scores.sort_values(ascending=False).to_dict()
        except Exception as e:
            info['shap_importance_error'] = str(e)
            shap_scores = pd.Series(0, index=X.columns)
    else:
        shap_scores = pd.Series(0, index=X.columns)

    mean_rank = (
        mi_scores.rank(ascending=False, method="min")
        + fi_scores.rank(ascending=False, method="min")
        + shap_scores.rank(ascending=False, method="min")
    ) / 3
    selected_features = mean_rank.sort_values().head(top_n).index.tolist()
    info['selected_features'] = selected_features

    return selected_features, info

##############################
# Smart Dynamic Sample Size (Super Cerdas, Hanya diatur di sini!)
##############################
def smart_dynamic_sample_size(
    n_rows: int,
    min_sample: Optional[int] = None,
    max_sample: Optional[int] = None,
    min_frac: Optional[float] = None,
    max_frac: Optional[float] = None
) -> int:
    """
    Menentukan jumlah sample representatif secara dinamis.
    HANYA fungsi ini yang menentukan sample size, abaikan input n_sample dari luar!
    """
    # Load defaults from config if not provided
    min_sample = min_sample if min_sample is not None else SAMPLING_CONFIG.get("min_sample", 100)
    max_sample = max_sample if max_sample is not None else SAMPLING_CONFIG.get("max_sample", 3000)
    min_frac = min_frac if min_frac is not None else SAMPLING_CONFIG.get("min_frac", 0.005)
    max_frac = max_frac if max_frac is not None else SAMPLING_CONFIG.get("max_frac", 0.1)

    # Hitung sample berbasis fraksi dan batas
    sample_size = int(n_rows * 0.03)
    sample_size = max(sample_size, int(n_rows * min_frac), min_sample)
    sample_size = min(sample_size, int(n_rows * max_frac), max_sample, n_rows)
    print(f"[SmartSample] n_rows={n_rows}, sample_size={sample_size}, min={min_sample}, max={max_sample}, min_frac={min_frac}, max_frac={max_frac}")
    return sample_size

##############################
# Smart Sampling with sklearn/imbalanced-learn
##############################
def detect_stratify_column(df: pd.DataFrame, target: Optional[str] = None, max_unique: int = 20) -> Optional[str]:
    candidates = []
    for col in df.select_dtypes(include=["object", "category", "int", "bool"]):
        if col == target:
            continue
        nunique = df[col].nunique()
        if 1 < nunique <= max_unique:
            candidates.append(col)
    return candidates[0] if candidates else None

def smart_sample(
    df: pd.DataFrame,
    n_sample: int,
    stratify_col: Optional[str] = None,
    balanced_col: Optional[str] = None,
    random_state: int = 42,
    cluster_cols: Optional[list] = None,
    n_clusters: int = 10
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    info = {}
    n_rows = len(df)

    # 1. Stratified sampling
    if stratify_col and stratify_col in df.columns:
        info['strategy'] = 'stratified'
        split = StratifiedShuffleSplit(n_splits=1, train_size=min(n_sample/n_rows, 1.0), random_state=random_state)
        for _, idx in split.split(df, df[stratify_col]):
            sample = df.iloc[idx]
        info['stratify_col'] = stratify_col
        return sample.reset_index(drop=True), info

    # 2. Balanced sampling (using imbalanced-learn)
    if balanced_col and balanced_col in df.columns and IMBLEARN_AVAILABLE:
        info['strategy'] = 'balanced'
        X = df.drop(columns=[balanced_col])
        y = df[balanced_col]
        sampler = RandomUnderSampler(random_state=random_state)
        X_res, y_res = sampler.fit_resample(X, y)
        sample = pd.concat([X_res, y_res], axis=1)
        if len(sample) > n_sample:
            sample = sample.sample(n=n_sample, random_state=random_state)
        info['balanced_col'] = balanced_col
        return sample.reset_index(drop=True), info

    # 3. Cluster sampling (for numeric data)
    if cluster_cols and all(col in df.columns for col in cluster_cols):
        info['strategy'] = 'cluster'
        X = df[cluster_cols].fillna(df[cluster_cols].mean())
        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
        clusters = kmeans.fit_predict(X)
        df["_cluster"] = clusters
        sample = df.groupby("_cluster", group_keys=False).apply(
            lambda x: x.sample(n=min(len(x), max(1, int(n_sample / n_clusters))), random_state=random_state)
        )
        info['n_clusters'] = n_clusters
        return sample.drop(columns=["_cluster"]).reset_index(drop=True), info

    # 4. Random sampling
    info['strategy'] = 'random'
    if n_sample < n_rows:
        sample = df.sample(n=n_sample, random_state=random_state)
    else:
        sample = df.copy()
    return sample.reset_index(drop=True), info

########################
# REPRESENTATIVITY VALIDATION (Evidently)
########################

def validate_sample_evidently(
    df_full: pd.DataFrame,
    df_sample: pd.DataFrame,
    columns: Optional[list] = None
) -> Dict[str, Any]:
    if not EVIDENTLY_AVAILABLE:
        return {"evidently": False, "msg": "Evidently not installed."}
    columns = columns or [col for col in df_full.columns if pd.api.types.is_numeric_dtype(df_full[col]) or df_full[col].dtype == "object"]
    tests = [TestColumnDistribution(column_name=col) for col in columns if col in df_sample.columns]
    suite = TestSuite(tests=tests)
    suite.run(reference_data=df_full, current_data=df_sample)
    result = suite.as_dict()
    summary = {}
    for test in result['tests']:
        summary[test['name']] = test['status']
    return {"evidently": True, "summary": summary, "success": all(v=="SUCCESS" for v in summary.values())}

##############################
# get_file_sample_df (ENTRY POINT. ABAIKAN n_sample DARI LUAR)
##############################
def get_file_sample_df(
    fpath: str,
    stratify_col: Optional[str] = None,
    balanced_col: Optional[str] = None,
    cluster_cols: Optional[list] = None,
    n_clusters: int = 10,
    random_state: int = 42,
    validate: bool = True,
    min_sample: Optional[int] = None,
    max_sample: Optional[int] = None,
    min_frac: Optional[float] = None,
    max_frac: Optional[float] = None
) -> Tuple[pd.DataFrame, dict]:
    """
    Fungsi utama: hanya sampling.py yang menentukan jumlah sample.
    Semua parameter sampling size dari kode lain diabaikan.
    Akan membaca default sampling config dari config.json jika parameter None.
    """
    df = pd.read_parquet(fpath, engine="pyarrow")
    n_rows = len(df)

    # HANYA sampling.py yang menentukan n_sample!
    n_sample = smart_dynamic_sample_size(
        n_rows,
        min_sample=min_sample,
        max_sample=max_sample,
        min_frac=min_frac,
        max_frac=max_frac
    )

    # Deteksi stratify_col otomatis jika belum diset
    stratify_auto = detect_stratify_column(df)
    if stratify_col is None and stratify_auto:
        stratify_col = stratify_auto

    # Sampling
    sample_df, info = smart_sample(
        df,
        n_sample=n_sample,
        stratify_col=stratify_col,
        balanced_col=balanced_col,
        random_state=random_state,
        cluster_cols=cluster_cols,
        n_clusters=n_clusters
    )

    # Validasi representativitas sample menggunakan Evidently
    if validate and EVIDENTLY_AVAILABLE:
        columns_to_check = sample_df.columns.tolist()[:10]
        eval_res = validate_sample_evidently(df, sample_df, columns=columns_to_check)
        info['evidently'] = eval_res
        if eval_res.get("success") is False:
            info['warning'] = "Sample mungkin kurang representatif pada beberapa kolom!"

    info.update({
        "input_file": fpath,
        "input_rows": n_rows,
        "sample_rows": len(sample_df),
        "n_sample_used": n_sample,
        "stratify_col_used": stratify_col
    })
    # Warning jika sample terlalu kecil (<0.5% data besar)
    warn = ""
    if len(sample_df) / n_rows < 0.005 and n_rows > 2000:
        warn = f"WARNING: Sample hanya {len(sample_df)} dari {n_rows} baris (<0.5%). Hasil EDA mungkin tidak representatif!"
        info['sampling_warning'] = warn
    return sample_df, info

##############################
# Unit Test & Example
##############################
def test_auto_feature_selector():
    df = pd.DataFrame({
        "x1": np.random.randn(100),
        "x2": np.random.rand(100),
        "cat": np.random.choice(['a', 'b', 'c'], 100),
        "target": np.random.randint(0, 2, 100)
    })
    feats, info = auto_feature_selector(df, target='target', task='classification', top_n=2)
    print("Selected features:", feats)
    print("Feature info:", info)

def test_get_file_sample_df():
    df = pd.DataFrame({
        "cat": ["a", "a", "b", "b", "c"]*20,
        "num": np.arange(100),
        "target": np.random.randint(0, 2, 100)
    })
    fpath = "test_data.parquet"
    df.to_parquet(fpath)
    sample, info = get_file_sample_df(fpath)
    print("Sample shape:", sample.shape)
    print("Sampling info:", info)
    os.remove(fpath)

if __name__ == "__main__":
    test_auto_feature_selector()
    test_get_file_sample_df()
    print("sampling.py tests OK")

8. schemas.py:

from pydantic import BaseModel
from typing import List, Dict, Optional, Any, Union

# --- Numeric Column Statistics ---
class NumericColStat(BaseModel):
    count: float
    mean: float
    std: float
    min: float
    q25: float
    q50: float
    q75: float
    max: float
    unique: int
    outlier_count: int
    hist_bins: List[float]
    hist_counts: List[int]
    skewness: float
    kurtosis: float
    zero_ratio: float
    neg_ratio: float
    pos_ratio: float
    mean_confidence_interval: Optional[List[float]] = None  # <--- Tambahan untuk confidence interval

# --- Categorical Column Statistics ---
class CategoricalColStat(BaseModel):
    unique: int
    top_freq: Dict[str, int]
    entropy: float
    rare_count: int
    rare_pct: float

# --- Schema/Expectations Auto-Suggestion ---
class ColumnSchemaExpectation(BaseModel):
    type: str
    min: Optional[Union[float, str]] = None
    max: Optional[Union[float, str]] = None
    n_unique: Optional[int] = None
    max_length: Optional[int] = None
    sample_values: Optional[List[Any]] = None

class AutoSchemaExpectation(BaseModel):
    columns: Dict[str, ColumnSchemaExpectation]

# --- Data Reference Example ---
class DataReference(BaseModel):
    min: Optional[float]
    max: Optional[float]
    sample: Optional[List[Any]]
    hist_bins: Optional[List[float]]
    hist_counts: Optional[List[int]]
    value_sample: Optional[List[Any]]

# --- Main EDA Result Schema ---
class EDAResult(BaseModel):
    file: str
    total_rows: int
    columns: List[str]
    columns_count: int
    duplicate_rows: int
    duplicate_pct: float
    missing_per_col: Dict[str, int]
    missing_pct_per_col: Dict[str, float]
    numeric: Dict[str, NumericColStat]
    categorical: Dict[str, CategoricalColStat]
    completeness_score: float
    confidence_score: float
    auto_schema_expectation: Optional[Dict[str, Any]] = None
    data_reference: Optional[Dict[str, Any]] = None
    sampling: Dict[str, Any]
    sampling_warning: Optional[str] = None  # <--- Tambahan: warning representativitas dari sampling.py
    generated_at: str
    meta: Optional[Dict[str, Any]] = None
    progress: Optional[Dict[str, Any]] = None

    # --- Optional: For advanced/LLM/GE/anomaly results ---
    profile: Optional[Any] = None            # (was profiling_dict)
    ge: Optional[Any] = None                 # (was ge_result)
    eda_insight: Optional[Any] = None        # <--- Untuk hasil insight/rekomendasi EDAReasonerFlexible
    anomaly: Optional[Any] = None
    warnings: Optional[Any] = None
    drift_alerts: Optional[Any] = None
    feature_interaction: Optional[Any] = None
    timeseries_profile: Optional[Any] = None
    plugin_results: Optional[Any] = None
    ge_suite_path: Optional[str] = None
    rangkuman_kolaborasi: Optional[Any] = None

# --- Flexible, backward compatible: all not strictly required except core EDA fields ---

9. utils.py:

import os
import json
import hashlib
import math
import datetime
from typing import Any, Dict, Optional, List
import pandas as pd
import numpy as np

from data_eda.eda_logger import save_eda_log  # Use best practice logger

# Define base and data directories (adjust as needed)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")
EXCLUDED_FILES = {"other_gdrive_meta.json", "file_progress.json"}

# --- REVISED: All log/metadata JSON files go to data_eda/data_log ---
DATA_LOG_DIR = os.path.join(BASE_DIR, "data_eda", "data_log")
os.makedirs(DATA_LOG_DIR, exist_ok=True)

def safe(val, default=None):
    """Return default if val is NaN, inf, None, empty, or null-like."""
    # --- Patch: handle numpy array (DeprecationWarning fix) ---
    if isinstance(val, np.ndarray):
        if val.size == 0:
            return default
        # For non-empty arrays, treat as value if needed
        if val.size == 1:
            return safe(val.item(), default)
        return val
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return default
    if isinstance(val, (np.floating, )):
        if np.isnan(val) or np.isinf(val):
            return default
    if pd.isnull(val):
        return default
    return val if val not in [None, "", [], {}, "null", "None"] else default

def load_json(path: str, default=None) -> Any:
    """Load JSON from file, return default on error."""
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"load_json error {e}", "file": path}, log_type="utils_error")
        return default

def save_json(path: str, obj: Any):
    """Save object as JSON to file."""
    def clean_json(obj):
        if isinstance(obj, dict):
            return {str(k): clean_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [clean_json(v) for v in obj]
        elif isinstance(obj, float):
            if math.isnan(obj) or math.isinf(obj):
                return None
            return obj
        elif isinstance(obj, (np.integer, )):
            return int(obj)
        elif isinstance(obj, (np.floating, )):
            return float(obj)
        elif isinstance(obj, (datetime.datetime, datetime.date, pd.Timestamp)):
            return str(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(clean_json(obj), f, indent=2, ensure_ascii=False)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"save_json error {e}", "file": path}, log_type="utils_error")

def calc_sha256_from_file(path: str) -> str:
    """Calculate SHA256 hash of a file."""
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"calc_sha256_from_file error {e}", "file": path}, log_type="utils_error")
        return ""

def clean_json(obj):
    """Recursively clean object for JSON serialization."""
    if isinstance(obj, dict):
        return {str(k): clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, (np.integer, )):
        return int(obj)
    elif isinstance(obj, (np.floating, )):
        return float(obj)
    elif isinstance(obj, (datetime.datetime, datetime.date, pd.Timestamp)):
        return str(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

def parse_iso_to_local(dt_str: str) -> str:
    """Convert ISO string to local time ISO string."""
    if not dt_str or not isinstance(dt_str, str):
        return ""
    dt_str = dt_str.replace("+00:00Z", "Z").replace("Z+00:00", "Z")
    try:
        if dt_str.endswith("Z"):
            dt_str = dt_str[:-1] + "+00:00"
        dt_utc = pd.to_datetime(dt_str, utc=True)
        dt_local = dt_utc.tz_convert(None).to_pydatetime().astimezone()
        return dt_local.replace(microsecond=0).isoformat()
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, "parse_iso_to_local", {"error": f"parse_iso_to_local error {e}", "value": dt_str}, log_type="utils_error")
        return ""

def parse_iso_to_utc(dt_str: str):
    """Convert ISO string to pandas UTC datetime."""
    if not dt_str or not isinstance(dt_str, str):
        return None
    dt_str = dt_str.replace("+00:00Z", "Z").replace("Z+00:00", "Z")
    try:
        if dt_str.endswith("Z"):
            dt_str = dt_str[:-1] + "+00:00"
        return pd.to_datetime(dt_str, utc=True)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, "parse_iso_to_utc", {"error": f"parse_iso_to_utc error {e}", "value": dt_str}, log_type="utils_error")
        return None

def get_all_parquet_files(data_dir=DATA_DIR) -> List[Dict]:
    """List all parquet files in data directory except excluded files."""
    files = []
    if not os.path.isdir(data_dir):
        return files
    for fname in os.listdir(data_dir):
        if fname in EXCLUDED_FILES or fname.startswith(".") or not fname.lower().endswith(".parquet"):
            continue
        fpath = os.path.join(data_dir, fname)
        if os.path.isfile(fpath):
            files.append({
                "name": fname,
                "path": fpath,
                "size": os.path.getsize(fpath),
                "modified": datetime.datetime.fromtimestamp(os.path.getmtime(fpath)).isoformat()
            })
    return files

def get_file_metadata():
    """Load and combine file metadata from meta and progress JSON files."""
    meta = load_json(META_FILE, default={})
    progress = load_json(PROGRESS_FILE, default={})
    meta_map = {}
    if isinstance(meta, dict) and "files" in meta:
        for f in meta["files"]:
            meta_map[f.get("name")] = f
    elif isinstance(meta, list):
        for f in meta:
            meta_map[f.get("name")] = f
    progress_map = {}
    if isinstance(progress, dict) and "progress" in progress:
        for f in progress["progress"]:
            progress_map[f.get("name")] = f
    elif isinstance(progress, list):
        for f in progress:
            progress_map[f.get("name")] = f
    return meta_map, progress_map

def enrich_files_with_metadata(files: List[Dict]):
    """Add meta and progress info to file dicts."""
    meta_map, progress_map = get_file_metadata()
    for f in files:
        fname = f["name"]
        f["meta"] = meta_map.get(fname, {})
        f["progress"] = progress_map.get(fname, {})
    return files

def get_df_metadata(df: pd.DataFrame) -> Dict[str, Any]:
    """Get dataframe metadata: dtype, min, max, null, unique, sample."""
    meta = {}
    for col in df.columns:
        ser = df[col]
        meta[col] = {
            "type": str(ser.dtype),
            "min": float(ser.min()) if pd.api.types.is_numeric_dtype(ser) else None,
            "max": float(ser.max()) if pd.api.types.is_numeric_dtype(ser) else None,
            "null_count": int(ser.isnull().sum()),
            "nunique": int(ser.nunique()),
            "sample": ser.dropna().unique()[:5].tolist(),
        }
    return meta

##############################
# Integrasi dengan sampling.py
##############################
def get_representative_sample(
    fpath: str,
    n_sample: int = 100,
    frac: float = None,
    stratify_col: str = None,
    balanced_col: str = None,
    cluster_cols: List[str] = None,
    n_clusters: int = 10,
    random_state: int = 42,
    validate: bool = True
):
    """
    Wrapper util untuk mengambil sample representatif dari file parquet menggunakan sampling.py.
    """
    try:
        from sampling import get_file_sample_df
        sample_df, info = get_file_sample_df(
            fpath=fpath,
            n_sample=n_sample,
            frac=frac,
            stratify_col=stratify_col,
            balanced_col=balanced_col,
            cluster_cols=cluster_cols,
            n_clusters=n_clusters,
            random_state=random_state,
            validate=validate
        )
        return sample_df, info
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, fpath, {"error": f"get_representative_sample error {e}", "file": fpath}, log_type="utils_error")
        return None, {"error": str(e)}

# --- Unit Tests (Run python utils.py for basic check) ---
def test_safe():
    assert safe(None, 0) == 0
    assert safe(float('nan'), 1) == 1
    assert safe("", 9) == 9
    assert safe(7, 9) == 7
    assert safe(np.array([]), 42) == 42
    assert safe(np.array([1.0]), 0) == 1.0

def test_load_json_and_save_json(tmp_path):
    test_path = tmp_path / "test.json"
    save_json(str(test_path), {"a": 1, "b": np.int64(5)})
    d = load_json(str(test_path))
    assert d == {"a": 1, "b": 5}

def test_get_df_metadata():
    df = pd.DataFrame({
        "num": [1, 2, 3, 4, 5, np.nan],
        "cat": ["a", "b", "b", "c", "c", "c"]
    })
    meta = get_df_metadata(df)
    assert "num" in meta and "cat" in meta
    assert meta["num"]["type"] == "float64"
    assert meta["cat"]["type"] == "object"

if __name__ == "__main__":
    test_safe()
    import tempfile
    from pathlib import Path
    p = tempfile.TemporaryDirectory()
    test_load_json_and_save_json(Path(p.name))
    test_get_df_metadata()
    print("utils.py tests OK")

9. feature_interaction.py:

import pandas as pd
import numpy as np

def correlation_report(df: pd.DataFrame, threshold=0.7):
    """
    Mendapatkan pasangan kolom dengan korelasi Pearson absolut di atas threshold.
    """
    corr = df.select_dtypes(include=[np.number]).corr(method="pearson").abs()
    pairs = []
    for col1 in corr.columns:
        for col2 in corr.columns:
            if col1 >= col2:
                continue
            val = corr.loc[col1, col2]
            if val > threshold:
                pairs.append({"pair": (col1, col2), "corr": float(val)})
    return pairs

def vif_report(df: pd.DataFrame, thresh=5.0):
    """
    Mendapatkan kolom dengan VIF (Variance Inflation Factor) di atas threshold.
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    X = df.select_dtypes(include=[np.number]).dropna()
    vifs = []
    if X.shape[1] > 1:
        for i, col in enumerate(X.columns):
            vif = variance_inflation_factor(X.values, i)
            if vif > thresh:
                vifs.append({"column": col, "vif": float(vif)})
    return vifs

def analyze_feature_interaction(df: pd.DataFrame, sample_if_large=True, sample_size=1000):
    """
    Analisis interaksi fitur (korelasi tinggi dan VIF) pada DataFrame.
    Akan otomatis sampling jika data sangat besar dan sample_if_large=True.
    """
    # --- Integrasi dengan sampling.py: sampling otomatis jika data besar ---
    if sample_if_large and len(df) > sample_size:
        try:
            from sampling import get_file_sample_df
            # Sampling hanya pada data numerik (agar efisien)
            num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if num_cols:
                tmp_df = df[num_cols]
                # Sampling, tanpa stratifikasi (karena hanya numerik)
                sampled_df, _ = get_file_sample_df(
                    fpath=None,
                    n_sample=sample_size,
                    frac=None,
                    stratify_col=None,
                    balanced_col=None,
                    cluster_cols=num_cols if len(num_cols) > 3 else None,
                    n_clusters=min(10, sample_size, len(tmp_df) // 100),
                    random_state=42,
                    validate=False
                ) if hasattr(get_file_sample_df, "__call__") else (tmp_df.sample(n=sample_size, random_state=42), {})
                df = sampled_df
        except Exception:
            # Jika sampling.py tidak tersedia, fallback: sample biasa jika data besar
            df = df.sample(n=sample_size, random_state=42)
    return {
        "high_correlation": correlation_report(df),
        "high_vif": vif_report(df)
    }

10. timeseries_profile.py:

import pandas as pd
import numpy as np

def find_timeseries_columns(df: pd.DataFrame):
    """
    Temukan kolom bertipe datetime di DataFrame.
    """
    dt_cols = [col for col in df.columns if np.issubdtype(df[col].dtype, np.datetime64)]
    return dt_cols

def seasonal_decompose_report(df: pd.DataFrame, col: str, period: int = 12):
    """
    Laporan dekomposisi musiman pada satu kolom time series.
    """
    from statsmodels.tsa.seasonal import seasonal_decompose
    results = {}
    ser = df[col].dropna().sort_index()
    if len(ser) < period * 2:
        return {"error": "Terlalu sedikit data untuk dekomposisi musiman"}
    try:
        result = seasonal_decompose(ser, period=period, model='additive')
        results["trend"] = result.trend.describe().to_dict() if hasattr(result.trend, "describe") else {}
        results["seasonal_strength"] = float(np.std(result.seasonal)) / np.std(ser)
        results["resid_std"] = float(np.std(result.resid))
    except Exception as e:
        results["error"] = str(e)
    return results

def autocorrelation_report(df: pd.DataFrame, col: str, lags=20):
    """
    Laporan autokorelasi (ACF) pada kolom time series.
    """
    from statsmodels.tsa.stattools import acf
    ser = df[col].dropna()
    if len(ser) < lags+1:
        return {"error": "Data terlalu pendek untuk ACF"}
    acfs = acf(ser, nlags=lags)
    return {"acf": acfs.tolist()}

def gap_analysis(df: pd.DataFrame, dt_col: str):
    """
    Analisis gap antar waktu pada kolom datetime.
    """
    ser = pd.to_datetime(df[dt_col]).dropna().sort_values()
    gaps = ser.diff().dt.total_seconds().dropna()
    large_gaps = gaps[gaps > gaps.median() * 2]
    return {
        "gap_median_sec": float(gaps.median()) if len(gaps) > 0 else 0.0,
        "num_large_gaps": int((gaps > gaps.median() * 2).sum()) if len(gaps) > 0 else 0,
        "large_gaps": large_gaps.tolist()[:10] if len(large_gaps) > 0 else []
    }

##############################
# Integrasi sampling.py
##############################
def analyze_timeseries(
    df: pd.DataFrame,
    sample_if_large: bool = True,
    sample_size: int = 500,
    use_sampling_py: bool = True
):
    """
    Analisis timeseries pada DataFrame. Jika data besar, otomatis sampling menggunakan sampling.py jika tersedia.
    """
    # --- Sampling jika data besar ---
    if sample_if_large and len(df) > sample_size:
        try:
            if use_sampling_py:
                from sampling import get_file_sample_df
                # Sampling tanpa stratifikasi (karena ini timeseries)
                sampled_df, _ = get_file_sample_df(
                    fpath=None,
                    n_sample=sample_size,
                    frac=None,
                    stratify_col=None,
                    balanced_col=None,
                    cluster_cols=None,
                    random_state=42,
                    validate=False
                )
                df = sampled_df
            else:
                df = df.sample(n=sample_size, random_state=42)
        except Exception:
            df = df.sample(n=sample_size, random_state=42)
    dt_cols = find_timeseries_columns(df)
    reports = {}
    for col in dt_cols:
        reports[col] = {
            "gap_analysis": gap_analysis(df, col),
            "autocorrelation": autocorrelation_report(df, col),
            "seasonal_decompose": seasonal_decompose_report(df, col)
        }
    return reports

##############################
# Unit Test
##############################
def test_timeseries_profile():
    # Buat data dummy dengan datetime dan tren
    dates = pd.date_range("2022-01-01", periods=120, freq="M")
    vals = np.sin(np.arange(120) / 12 * 2 * np.pi) + np.random.normal(0, 0.1, 120)
    df = pd.DataFrame({"ts": dates, "y": vals})
    df = df.set_index("ts")
    # Kolom y akan dianggap numerik, ts sebagai index (pastikan ada kolom datetime)
    df2 = df.copy()
    df2.reset_index(inplace=True)
    res = analyze_timeseries(df2)
    print("Timeseries profile:", res)
    assert isinstance(res, dict)
    print("timeseries_profile.py tests OK")

if __name__ == "__main__":
    test_timeseries_profile()

11. data_drift.py:

import os
import json
import numpy as np

# --- REVISED: Save/load baseline_stats.json in data_eda/data_log ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_LOG_DIR = os.path.join(BASE_DIR, "data_eda", "data_log")
os.makedirs(DATA_LOG_DIR, exist_ok=True)
BASELINE_PATH = os.path.join(DATA_LOG_DIR, "baseline_stats.json")

def save_baseline_stats(stats: dict, baseline_path: str = BASELINE_PATH):
    """
    Simpan statistik baseline ke file JSON.
    """
    with open(baseline_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

def load_baseline_stats(baseline_path: str = BASELINE_PATH):
    """
    Load statistik baseline dari file JSON.
    """
    if not os.path.exists(baseline_path):
        return None
    with open(baseline_path, "r", encoding="utf-8") as f:
        return json.load(f)

def compute_stats(df):
    """
    Hitung statistik kolom numerik utama untuk deteksi drift.
    """
    stats = {}
    for col in df.columns:
        if np.issubdtype(df[col].dtype, np.number):
            arr = df[col].dropna()
            if len(arr) == 0:
                continue
            mean = float(arr.mean())
            std = float(arr.std())
            minv = float(arr.min())
            maxv = float(arr.max())
            # Outlier = nilai lebih dari 3 std dari mean
            outlier_rate = float(((np.abs(arr - mean) > 3 * std).sum()) / len(arr)) if std > 0 else 0.0
            stats[col] = {
                "mean": mean,
                "std": std,
                "min": minv,
                "max": maxv,
                "outlier_rate": outlier_rate
            }
    return stats

def detect_drift(current_stats, baseline_stats, threshold=0.15):
    """
    Deteksi drift pada statistik kolom numerik berdasarkan baseline.
    threshold default: 15% perubahan relatif.
    """
    alerts = []
    for col in current_stats:
        if col in baseline_stats:
            for k in ["mean", "std", "outlier_rate"]:
                base = baseline_stats[col].get(k)
                curr = current_stats[col].get(k)
                if base is None or curr is None or base == 0:
                    continue
                delta = abs(curr - base) / abs(base)
                if delta > threshold:
                    alerts.append(
                        f"Drift pada kolom '{col}' ({k}): baseline={base:.3f}, sekarang={curr:.3f}, delta={delta:.2%}"
                    )
    return alerts

##############################
# Integrasi dengan sampling.py (pipeline EDA)
##############################
def compute_and_check_drift(
    df,
    threshold=0.15,
    baseline_path: str = BASELINE_PATH,
    save_if_none=True
):
    """
    Hitung statistik data, cek drift terhadap baseline (jika ada).
    Jika baseline belum ada dan save_if_none=True, baseline akan dibuat dari data ini.
    Return: tuple (drift_alerts, current_stats, baseline_stats)
    """
    current_stats = compute_stats(df)
    baseline_stats = load_baseline_stats(baseline_path)
    if baseline_stats:
        drift_alerts = detect_drift(current_stats, baseline_stats, threshold=threshold)
    else:
        drift_alerts = []
        if save_if_none:
            save_baseline_stats(current_stats, baseline_path)
    return drift_alerts, current_stats, baseline_stats

# Example usage in EDA pipeline:
# drift_alerts, current_stats, baseline = compute_and_check_drift(df)

##############################
# Unit Test
##############################
def test_data_drift():
    import pandas as pd
    df = pd.DataFrame({
        "A": np.random.normal(0, 1, 1000),
        "B": np.random.normal(10, 5, 1000)
    })
    # Save baseline
    stats = compute_stats(df)
    save_baseline_stats(stats)
    # Modify data for drift detection
    df2 = df.copy()
    df2["A"] += 1.5  # introduce mean shift
    drift_alerts, cur_stats, base_stats = compute_and_check_drift(df2)
    print("Drift alerts:", drift_alerts)
    assert isinstance(drift_alerts, list)
    assert "Drift pada kolom 'A' (mean)" in " ".join(drift_alerts) or len(drift_alerts) > 0
    print("data_drift.py tests OK")

if __name__ == "__main__":
    test_data_drift()
